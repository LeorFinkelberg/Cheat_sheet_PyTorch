\documentclass[%
	11pt,
	a4paper,
	utf8,
	%twocolumn
		]{article}	

\usepackage{style_packages/podvoyskiy_article_extended}


\begin{document}
\title{Приемы работы с библиотекой PyTorch}

\author{}

\date{}
\maketitle

\thispagestyle{fancy}

\tableofcontents

\section{Вводные замечания}

Чаще всего цикл обучения модели реализуют в виде обычного цикла \verb|for| Python. Оптимизатор, доступный в модуле \verb|torch.optim| PyTorch, который будет отвечать за обновление параметров. 

По умолчанию в PyTorch используется модель немедленного выполнения (eager mode). Как только интерпретатор Python выполняет инструкцию, связанную с PyTorch, базовая реализация C++ или CUDA сразу же производит соответствующую операцию.

PyTorch также предоставляет возможности предварительной компиляции моделей с помощью TorchScript. Используя TorchScript, PyTorch может преобразовать модель в набор инструкций, которые можнро независимо вызывать из Python, допустим, из программ на C++ или на мобильных устройствах. Это можно считать своего рода виртуальной машиной с ограниченным набором инструкций, предназначенным для опреаций с тензорами. Экспортировать модель можно либо в виде TorchScript для использования со средой выполнения Python, либо в стандартизированном формате ONNX (платформонезависимый формат описания моделей).

Сети среднего размера могут потребовать от нескольких часов до нескольких дней для обучения с нуля на больших реальных наборах данных на рабочих станциях с хорошим GPU \cite[\strbook{48}]{pytorch-2022}. Длительность обучения можно сократить за счет использования на одной машине нескольких GPU или даже еще сильнее -- на кластере машин, оснащенных несколькими GPU. 

Для примера создадим сеть AlexNet
\begin{lstlisting}[
style = ironpython,
numbers = none	
]
# TorchVision включает несколько лучших нейросетевых архитектур для машинного зрения
from torchvision import models

alexnet = models.AlexNet()
\end{lstlisting}

Подав на вход \verb|alexnet| данные четко определенного размера, мы выполним прямой проход (forward pass) по сети, при котором входной сигнал пройдет через первый набор нейронов, выходные сигналы которых будут поданы на вход следующего набора нейронов, и так до самого итогового выходного сигнала. На практике это означает, что при наличии объекта \verb|input| нужного типа можно произвести прямой проход с помощью оператора \verb|ouput = alexnet(input)|.

Но если мы так поступим, то получим мусор. А все потому, что сеть не была инициализрована: ее веса, числа, с которыми складываются и на которые умножаются входные сигналы, не были обучены на чем-либо, сеть сама по себе -- чистый (или, точнее, сказать случайный) лист. Необходимо либо обучить ее с нуля, либо загрузить веса, полученные в результате предыдущего обучения \cite[\strbook{58}]{pytorch-2022}.

В \verb|models| названия в верхнем регистре соответствуют классам, реализующим популярные архитектуры, предназначенные для машинного зрения. С другой стороны, названия в нижнем регистре соответствуют функциям, создающим экземпляры моделей с заранее определенным количеством слоев и нейронов, а также, возможно, скачивающие и загружающие в них предобученные веса.

Для того чтобы привести входные изображения к нужному размеру, а их значения (цвета) примерно в один числовой диапазон, можно воспользоваться преобразованиями модуля \verb|torchvision|
\begin{lstlisting}[
style = ironpython,
numbers =  none
]
from torchvision import transforms

# это функция
preprocess = transformers.Compose([
    transformers.Resize(256),
    transformers.CenterCrop(224),
    transformers.ToTensor(),
    transformers.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    )
])
\end{lstlisting}

Здесь описана функция \verb|preprocess|, масштабирующую входное изображение до размера $256 \times 256$, обрезающую его до $224 \times 224$ по центру, преобразующую в тензор (многомерный массив PyTorch: в данном случае трехмерных массив, содержащий цвет, высоту и ширину) и нормализующую его компоненты RGB (красный, зеленый, синий) до заданных среднего значения и стандартного отклонения.

Если мы хотим получить от сети осмысленные ответы, все это должно соответствовать данным, полученным сетью во время обучения.

Процесс выполнения обученной модели на новых данных в сфере глубокого обучения называется \emph{выводом} (inference). Для выполнения вывода необходимо перевести сеть в режим \verb|eval|
\begin{lstlisting}[
style = ironpython,
numbers = none
]
resnet.eval()
\end{lstlisting}

Если забыть сделать это, некоторые предоубченные модели, например включающие нормализацию по мини-батчам и дропаут, не дадут никаких осмысленных результатов просто по причине их внутреннего устройства. Теперь, после установки режима \verb|eval|, можно выполнять вывод
\begin{lstlisting}[
style = ironpython,
numbers = none
]
out = resnet(batch_t)
# получается что-то вроде степени уверенности модели в конкретном предсказании
percentage = torch.nn.functional.softmax(out, dim=1)[0] * 100
\end{lstlisting}

Успешность работы сети во многом зависит от наличия соответствующих объектов в обучающем наборе данных. Если подать нейронной сети нечто выходящее за рамки обучающего набора данных, вполне возможно, что она достаточно уверенно вернет неправильный ответ \cite[\strbook{64}]{pytorch-2022}.

Сеть представляет собой всего лишь каркас, а вся суть -- в весовых коэффициентах \cite[\strbook{69}]{pytorch-2022}.

\subsection{Torch Hub}

Автору, чтобы опубликовать модель через механизм Torch Hub, необходимо всего лишь поместить файл \verb|hubconf.py| в корневой каталог репозитория GitHub. Структура очень проста
\begin{lstlisting}[
style = bash,
numbers = none
]
# необязательный список модулей, от которых зависит данный код
dependencies = ["torch", "math"]

# одна или несколько функций, открываемых пользователям в качестве входных точек репозитория. Эти функции должны инициализировать модели в соответствии с агрументами и возвращать их
def some_entry_fn(*args, **kwargs):
    model = build_some_model(*args, **kwargs)
    return model
    
def another_entry_fn(*args, **kwargs):
    model = build_another_model(*args, **kwargs)
    return model
\end{lstlisting}

Теперь интересные предобученные модели можно искать в репозиториях GitHub, содержащих файл \verb|hubconf.py|, зная сразу же, что их можно будет загрузить с помощью модуля torch.hub.
\begin{lstlisting}[
style = bash,
numbers = none	
]
import torch
from torch import hub

resnet18_model = hub.load(
    "pytorch/vision:main",  # название и ветка репозитория GitHub
    "resnet18",  # название точки входа
    pretrained=True  # ключевой аргумент
)
\end{lstlisting}

Приведенный код скачивает копию состояния ветки \verb|main| репозитория \verb|pytorch/vision|, вместе с весовыми коэффициентами в локальный каталог (по умолчанию \verb|.torch/hub| в домашнем каталоге) и выполняет функцию точки входа \verb|resnet18|, возвращающую созданный экземпляр модели.

\section{Тензор}

В контексте глубокого обучения тензоры связаны с обобщением векторов и матриц на произвольную размерность. Другими словами, речь идет о многомерных массивах.

По сравнению с массивами NumPy тензоры PyTorch обладают несколькими потрясающими способностями, например возвожностью чрезвычайно быстро выполнять операции на графических процессорах, умением распределять операции по нескольким устройствам или машинам, а также отслеживать породивший их граф вычислений. 

\emph{\color{blue}Тензоры PyTorch и массивы NumPy} это представления над (обычно) \emph{\color{blue}непрерывными блоками памяти}, содержащими распакованные (unboxed) числовые типы данных Си, \emph{а не объекты Python} \cite[\strbook{83}]{pytorch-2022}.

Пример тензора
\begin{lstlisting}[
style = bash,
numbers = none
]
img_t = torch.randn(3, 5, 5)
batch_t = torch.tensor(2, 3, 5, 5)  # [батч, каналы, строки, столбцы]
\end{lstlisting}

Иногда каналы RGB размещаются в измерении 0, а иногда -- в измерении 1. Но обобщение можно производить путем отсчета с конца: \emph{каналы} всегда расположены в измерении -3, третьем с конца. 
\begin{lstlisting}[
style = bash,
numbers = none
]
img_gray_naive = img_t.mean(-3)
batch_gray_naive = batch_t.mean(-3)
img_gray_naive.shape, batch_gray_naive.shape  # (torch.Size([5, 5]), torch.Size([2, 5, 5]))
\end{lstlisting}

PyTorch автоматически добавляет в начало измерение размером 1. Эта функция называется \emph{транслированием} (broadcasting). \verb|batch_t| формы $(2, 3, 5, 5)$ умножается на \verb|unsqueeze_weights| формы $(3, 1, 1)$, в результате чего получается тензор формы $(2, 3, 5, 5)$, в котором затем можно сложить третье измерение с конца (три канала).
\begin{lstlisting}[
style = bash,
numbers = none
]
weights = torch.tensor([0.2126, 0.7152, 0.0722])  # torch.Size([3])
unsqueezed_weights = weights.unsqueeze(-1).unsqueeze_(-1)  # torch.Size([3, 1, 1])
img_weights = (img_t * unsqueezed_weights)
batch_weights = (batch_t * unsqueezed_weights)
\end{lstlisting}

В PyTorch 1.3 добавилась экспериментальная возможность \emph{именновых тензоров}. У функций создания тензоров, например \verb|tensor| и \verb|rand|, есть аргумент \verb|names|. В качестве аргумента \verb|names| должна передаваться последовательность строковых значений
\begin{lstlisting}[
style = bash,
numbers = none
]
weights_named = torch.tensor([0.2126, 0.7152, 0.0722], names=["channels"])
\end{lstlisting}

При необходимости добавить названия в имеющийся тензор (не меняя существующие) можно вызвать его метод \verb|refine_names|. Аналогично доступу по индексу с помощью многоточия можно пропускать любое количество измерений. С помощью родственного ему метода \verb|rename| можно также переопределять или удалять (путем передачи \verb|None|) уже существующие названия
\begin{lstlisting}[
style = bash,
numbers = none
]
img_named = img_t.refine_names(..., "channels", "rows", "columns")
\end{lstlisting}

Метод \verb|align_as| возвращает тензор, в котором добавлены недостающие измерения, а уже существующие переставлены в нужном порядке \cite[\strbook{89}]{pytorch-2022}
\begin{lstlisting}[
style = bash,
numbers = none
]
weights_named.shape  # torch.Size([3])
img_named.shape  # torch.Size([3, 5, 5])
weights_aligned = weights_named.align_as(img_named)
weights_aligned.shape  # torch.Size([3, 1, 1])
\end{lstlisting}

Функции, принимающие на входе аргументы для измерений, также позволяют указывать поименованные измерения
\begin{lstlisting}[
style = bash,
numbers = none
]
(img_named * weights_aligned).sum("channels")
# то же самое
(img_named * weights_aligned).sum(0)
\end{lstlisting}

При попытке сочетать измерения с различными названиями выдается сообщение об ошибке
\begin{lstlisting}[
style = bash,
numbers = none
]
# img_named[..., :3] то же самое, что и img_named[:, :, :3]
gray_named = (img_named[..., :3] * weights_named).sum("channels")  # Ошибка, т.к. размерности не совпадают
# а так можно
(img_named[:, :, :3] * weights_named).sum("channels")
\end{lstlisting}

При необходимости использовать тензоры не только в функциях, работающих с поименованными тензорами, необходимо удалить названия, установив их в \verb|None|
\begin{lstlisting}[
style = bash,
numbers = none
]
img_named.rename(None)
\end{lstlisting}

\subsection{Типы элементов тензоров}

Использовать стандартные типы данных Python не рекомендуется по нескольким причинам \cite[\strbook{90}]{pytorch-2022}:
\begin{itemize}
	\item \emph{Числовые значения в Python являются \underline{объектами}}. В то время как число с плавающей запятой требует для представления в компьютере только 32 бита, Python преобразует его в \emph{\color{red}полноценный объект Python} с подсчетом ссылок и т.д. Эта операция, которая называется \emph{упаковой} (boxing), не является проблемой при хранении небольшого количества числовых значений, но выделять память для миллионов таких объектов -- совершенно нерационально.
	
	\item \emph{Списки в Python предназнены для хранения последовательных наборов \underline{объектов}}. В них нет операций для быстрого вычисления скалярного произведения двух векторов или их суммирования. Кроме того, \emph{списки Python} не оптимизируют размещение своего содержимого в памяти, поскольку представляют собой \emph{наборы указателей на объекты Python} (любые, не только числовые значения) с доступом по индексу. Наконец, списки Python одномерны, и, хотя можно создавать списки списков, это тоже нерационально.
	
	\item \emph{Интерпретатор Python работает медленно по сравнению с оптимизированным, скомпилированным кодом}.
\end{itemize}

Вычисления в нейронных сетях обычно производятся над 32-битными значениями с плавающей запятой. Более высокая точность, например 64-битные значения, обычно не повышает безошибочность модели, но требует больше памяти и вычислительного времени. Нативная поддержка типа данных с половинной точностью -- 16-битных значений с плавающей запятой -- в стандартных CPU обычно отсутствует, зато предоставляется современными GPU. При необходимости можно перейти на \emph{половинную точность} для снижения объема занимаемой памяти нейросетевой модели без особого влияния на степень безошибочности \cite[\strbook{92}]{pytorch-2022}.

Привести результат функции создания тензора к нужному типу с помощью соответствующего метода приведения типов, можно так
\begin{lstlisting}[
style = bash,
numbers = none
]
torch.zeros(10, 2).double()
torch.ones(10, 2).short()
\end{lstlisting}

Или с помощью более удобного метода \verb|.to()|
\begin{lstlisting}[
style = bash,
numbers = none	
]
torch.zeros(10, 2).to(torch.double)
torch.ones(10, 2).to(dtype=torch.short)
\end{lstlisting}

Память под значения в тензорах выделяется непрерывными фрагментами памяти под управлением экземпляров \verb|torch.Storage|. Хранилище представляет собой одномерный массив числовых данных, то есть непрерывный фрагмент памяти, содержащий числа заданного типа, например \verb|float| (32-битные значения, выражающие числа с плавающей запятой) или \verb|int64| (64-битные значения, выражющие целые числа). Экземпляр класса \verb|Tensor| PyTorch -- это представление подобного экземпляра \verb|Storage| с возможностью доступа к хранилищу по индексу через указание сдвига и шага по каждому измерению.

Хранилище \emph{всегда} представляет собой \emph{одномерный} массив вне зависимости от размерности каких-либо ссылающихся на него тензоров.

Методы, имена которых заканчиваются на символ <<\verb|_|>>, как в \verb|zero_|, указывают, что метод работает с заменой на месте (in place), изменяя входный данные вместо того, чтобы создавать новый выходной тензор и возвращать его. Метод \verb|zero_| обнуляет все элементы входного тензора. Все методы, в конце названия которых нет символа подчеркивания, оставляют исходный тензор неизменным и вместо этого возвращают новый.

Для транспонирования двумерных тензоров используется метод \verb|.t()|. Но в PyTorch транспорировать можно не только матрицы. Можно транспонировать многомерный массив, и для этого достаточно указать два измерения, по которым нужно произвести транспонирование (зеркально отражая форму шага)
\begin{lstlisting}[
style = bash,
numbers = none
]
somt_t = torch.ones(3, 4, 5)
transpose_t = some_t.transpose(0, 2)
some_t.shape  # torch.Size([3, 4, 5])
transpose_t.shape  # torch.Size([5, 4, 3])
\end{lstlisting}

Любой из тензоров PyTorch можно перенести на (один из) GPU системы для массово-параллельных быстрых вычислений.

Помимо \verb|dtype|, класс \verb|Tensor| предоставляет атрибут \verb|device|, который описывает, где на компьютере размещаются данные тензора. 
\begin{lstlisting}[
style = bash,
numbers = none
]
points_gpu = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]], device="cuda")
\end{lstlisting}

Вместо этого можно скопировать созданный в CPU тензор на GPU с помощью метода \verb|to|
\begin{lstlisting}[
style = bash,
numbers = none	
]
points_gpu = points.to(device="cuda")
\end{lstlisting}

При этом возвращается новый тензор с теми же числовыми данными, но хранящийся в \emph{памяти GPU}, {\color{red}а не в обычной \emph{оперативной памяти системы}} \cite[\strbook{105}]{pytorch-2022}.

Если на нашей машине более одного GPU, можно также указать, на каком именно GPU размещать тензор, передав отсчитываемый с нуля целочисленный номер GPU на машине, вот так
\begin{lstlisting}[
style = bash,
numbers = none	
]
points_gpu = points.to(device="cuda:0")

# умножение выполняется на CPU
points = 2 * points
# умножение выполняется на GPU
points_gpu = 2 * points.to(device="cuda")
\end{lstlisting}

Отметим, что тензор \verb|points_gpu| не передается обратно в CPU после вычисления результата. Вот что происходит в этой строке:
\begin{itemize}
	\item Тензор \verb|points| копируется в GPU.
	
	\item Выделяется память в GPU под новый тензор, в котором будет храниться результат умножения.
	
	\item Возвращается обращение к этому GPU-тензору.
\end{itemize}

Следовательно, если мы прибавим к результату константу
\begin{lstlisting}[
style = bash,
numbers = none
]
points_gpu = points_gpu + 4
\end{lstlisting}
операция сложения будет по-прежнему производиться в GPU и никакой информации в CPU передаваться не будет (если мы не будем выводить полученный тензор на экран или обращаться к нему). Для переноса тензора обратно в CPU необходимо указать в методе \verb|.to()| аргумент \verb|cpu|
\begin{lstlisting}[
style = bash,
numbers = none
]
points_gpu.to(device="cpu")
\end{lstlisting}

Можно также для получения того же результата воспользоваться сокращенными методами \verb|.cpu()| и \verb|.cuda()| вместо метода 
\begin{lstlisting}[
style = bash,
numbers = none
]
points_gpu = points.cuda()
points_gpu = points.cuda(0)
points_gpu.cpu()
\end{lstlisting}

Стоит упомянуть, что с помощью метода \verb|.to()| можно менять тип данных и их место размещения одновременно, указав в качестве аргументов \verb|device| и \verb|dtype|.

Тензоры PyTorch можно очень эффективно преобразовать в массивы NumPy и наоборот. Благодаря этому можно воспользоваться огромными объемами функциональности экосистемы Python, основанной на типах массивов NumPy. Подобная совместимость с массивами NumPy, не требующая копирования кода, возможна благодаря работы системы хранения с буферным протоколом Python.

В разреженных тензорах хранятся только ненулевые значения, а также информация об индексах.

Для сериализации обхектов-тензоров PyTorch использует <<за кулисами>> \verb|pickle|, а также специализированный код сериализации для хранилища. Вот как можно сохранить наш тензор \verb|points| в файл \verb|ourpoints.t|
\begin{lstlisting}[
style = bash,
numbers = none
]
torch.save(points, "./data/p1ch3/ourpoints.t)
\end{lstlisting}

Либо можно передать файловый дескриптор файла вместо названия
\begin{lstlisting}[
style = bash,
numbers = none
]
with open("./data/p1ch3/ourpoints.t", mode="wb") as f:
    torch.save(points, f)
\end{lstlisting}

Загрузка тензора \verb|points| обратно также выполняется одной строкой кода
\begin{lstlisting}[
style = bash,
numbers = none
]
points = torch.load("./data/p1ch3/ourpoints.t")
\end{lstlisting}
что эквивалентно
\begin{lstlisting}[
style = bash,
numbers = none
]
with open("./data/p1ch3/ourpoints.t", mode="rb") as f:
    points = torch.load(f)
\end{lstlisting}

И хотя подобным образом можно быстро сохранять тензоры, если нужно загрузить их только в PyTorch, сам по себе формат файла не отличается совместимостью: прочитать тензор с помощью какого-либо еще ПО, помимо PyTorch, не получится.

HDF5 -- переносимый, широко поддерживаемый формат представления сериализованных многомерных массивов, организованный в виде вложенного ассоциативного массива типа <<ключ--значение>>. Python поддерживает формат HDF5 благодаря библиотеке \verb|h5py|, принимающей и возвращающей данные в виде массивов NumPy.
\begin{lstlisting}[
style = bash,
numbers = none
]
import h5py

f = h5py.File("./ourpoints.hdf5", "w")
dset = f.create_dataset("coords", data=points.numpy())
f.close()
\end{lstlisting}

Здесь \verb|"coords"| -- это ключ для файла в формате HDF5. В HDF5 интересна возможность индексации набора данных на диске и обращения только к нужным нам элементам. 
\begin{lstlisting}[
style = bash,
numbers = none
]
f = h5py.File("./ourpoints.hdf5", "r")
dset = f["coords"]
torch.from_numpy(dset[-2:])
f.close()
\end{lstlisting}

\subsection{Представление реальных данных с помощью тензоров}

\subsubsection{Работа с изображениями}

Изображения представляются в виде набора скалярных значений расположенных на равномерной сетке с высотой и шириной (в пикселях), например, по одному скалярному значению на каждую точку сетки (пиксель) для изображения в оттенках серого или несколько скалярных значений на каждую точку сетки для представления различных цветов.

Отражающие значения для различных пикселей скаляры обычно кодируются 8-битными целыми числами, как в бытовых фотоаппаратах. В медицинских, научных и промышленных приложениях нередко встречается более высокая точность, например 12- или 16-битная, для расширения диапазона или повышения чувствительности в случаях, когда пиксель отражает информацию о физическом свойстве, например о плотности костной ткани, температуре или глубине.

Загрузить изображение можно так
\begin{lstlisting}[
style = bash,
numbers = none	
]
import imageio

img_arr = imageio.imread("./bobby.jpg")
img_arr.shape  # (720, 1280, 3)
\end{lstlisting}

Модули PyTorch, работающие с изображениями, требуют от тензоров измерений $C \times H \times W$ (каналы, высота и ширина).

Для получения нужной нам схемы расположения можно воспользоваться методом \verb|permute| тензора, указав в качестве параметров старые измерения для каждого из новых.
\begin{lstlisting}[
style = bash,
numbers = none	
]
img = torch.from_numpy(img_arr)
out = img.permute(2, 0, 1)
\end{lstlisting}

Эта операция \underline{не копирует} данные тензора, вместо этого \verb|out| \emph{использует то же самое хранилище}, что и \verb|img|, только меняя информацию о размере и шаге на уровне тензора.

Несколько более эффективная альтернатива использованию для создания тензора \verb|stack| -- выделить заранее память под тензор нужного размера, а затем заполнить его загруженными из каталога изображениями следующим образом
\begin{lstlisting}[
style = bash,
numbers = none
]
batch_size = 3
batch = torch.zeros(batch_size, 3, 256, 256, dtype=torch.uint8)
\end{lstlisting}

Батч будет состоять из трех RGB-изображений по 256 пикселей высотой и 256 пикселей шириной. 

Нейронные сети демонстрируют наилучшее качество обучения, когда входные данные находятся в диапазоне примерно от 0 до 1 или от -1 до 1.

Никаких принципиальных различий между тензорами, содержащими объемные пространственные данные и данные изображения, нет. Просто появляется дополнительное измерение, глубина, вслед за измерением каналов, и получается 5-мерный тензор формы $N \times C \times D \times H \times W$.

Загрузим пример КТ-снимка с помощью функции \verb|volread| из модуля \verb|imageio|, принимающий в качестве аргумента каталог и собирающей все файлы в формате DICOM (Digital Imaging and Communications in Medicine) в трехмерный массив NumPy
\begin{lstlisting}[
style = bash,
numbers = none
]
import imageio

dir_path = ".../2-LUNG 3.0B70f-04083"
vol_arr = imageo.volread(dir_path, "DICOM")
vol_arr.shape  # (99, 512, 512)

vol = torch.from_numpy(vol_arr).float()
vol = torch.unsqueeze(vol, 0)
vol.shape  # torch.Size([1, 99, 512, 512]): каналы, глубина, высота, ширина
\end{lstlisting}

При вызове метода \verb|.view()| на тензоре возвращает новый тензор с другими размерностью и шагами \emph{без изменения хранилища}. Это позволяет перегруппировать тензор прктически без затрат, поскольку \emph{никакие данные копировать не нужно}.

Нормализацию данных к отрезку $[0; 1]$ или $[-1; 1]$ желательно производить \emph{для всех количественных величин}, таких как <<температура>> (это полезно для процесса обучения) \cite[\strbook{137}]{pytorch-2022}.

По завершении обучения алгоритм способен генерировать правильные выходные сигналы при получении новых данных, \emph{достаточно схожих} со входными данными, на которых он обучался. В случае глубокого обучения этот процесс работает даже тогда, когда входные данные и требуемые выходные сгиналы \emph{далеки} друг от друга: когда они относятся к различным предметным областям.

У нас есть модель с неизвестными значениями параметров и нужно получить оценку этих параметров, которая бы минимизировала расхождение между предсказанными выходными сигналами и измеренными значениями (ошибка). Целью процесса оптимизации должен быть поиск таких параметров модели, которые минимизировали бы функцию потерь.

Отдельная итерация обучения, во время которой обновляются параметры для всех обучающих примеров данных, называется \emph{эпохой}.

Градиенты по параметрам модели должны быть одного порядка \cite[\strbook{168}]{pytorch-2022}.

Аргумент \verb|requires_grad| указывает PyTorch отслеживать целое семейство тензоров. Если функции дифференцируемые (как большинство операций над тензорами PyTorch), величина производной будет автоматически занесена в атрибут \verb|.grad|.

Количество тензоров с параметром \verb|requires_grad|, установленным в \verb|True| аргументом, и композиции функций может быть любым. В этом случае PyTorch вычисляет производные функции потерь по всей цепочке функций (графу вычислений) и накапливает их значения в атрибутах \verb|.grad| этих тензоров (узлы этого графа).

При вызове \verb|.backward()| производные \emph{накапливаются} в узлах-листьях. {\color{blue}Необходимо \emph{явным образом обнулять градиенты} после обновления параметров на их основе}. Так что, если \verb|backward| вызывался ранее, потери оцениваются опять, \verb|backward| вызывается снова, после чего накапливаются градиенты во всех листьях графа, то есть суммируются с вычисленными на предыдущей итерации, в результате чего неправильное значение градиента \cite[\strbook{173}]{pytorch-2022}.

Чтобы предотвратить подобное, необходимо \emph{явным образом обнулять градиенты} на каждой итерации.

\begin{lstlisting}[
style = bash,
numbers = none
]
def training_loop(n_epochs, learning_rate, params, t_u, t_c):
    for epoch in range(1, n_epochs + 1):
        if params.grad. is not None:
            params.grad.zero_()
            
        t_p = model(t_u, *params)
        loss = loss_fn(t_p, t_c)
        loss.backward()  # выполяем обратный проход и вычисляем градиенты
        
        with torch.no_grad():
            params -= learning_rate * params.grad
            
        if epoch % 500 == 0:
            print(...)
\end{lstlisting}

При вызове \verb|loss.backward()| PyTorch обходит граф в обратном порядке, вычисляя градиенты. Контекст \verb|torch.no_grad()| означает, что механизм автоматического вычисления градиента игнорирует внутренности блока \verb|with|: то есть не добавляет ребра в граф прямого прохода.

У каждого оптимизатора доступны два метода: \verb|zero_grad| и \verb|step|. Метод \verb|zero_grad| обнуляет атрибут \verb|grad| всех передаваемых оптимизатору параметров при его создании. А метод \verb|step| обновляет значения параметров в соответствии с реализуемой конкретным оптимизатором стратегией оптимизации.
\begin{lstlisting}[
style = bash,
numbers = none	
]
def training_loop(n_epochs, optimizer, params, t_u, t_c):
    for epoch in range(1, n_epochs + 1):
        t_p = model(t_u, *params)
        loss = loss_fn(t_p, t_c)
        
        # обнуляем градиент, потому как в противном случае
        # производные накапливались бы в узлах-листьях графа вычислений
        optimizer.zero_grad()
        loss.backward()  # обратный проход по графу; вычисляем градиенты
        optimizer.step()  # обновляем параметры модели
        
        if epoch % 500 == 0:
            print(...)
            
    return params
\end{lstlisting}

Очень гибкая модель с большим количеством параметров стремится к минимизации функции потерь в точках данных и нет никаких гарантий, что она будет себя вести нужным образом \emph{вдали} или \emph{между} точками данных \cite[\strbook{180}]{pytorch-2022}.

Потери на обучающем наборе данных показывают, можно ли вообще подгнать нашу модель к этому обучающему набору данных - другими словами, достаточны ли \emph{разрешающие возможности} (capacity) этой модели для обработки содержащейся в данных информации \cite[\strbook{182}]{pytorch-2022}.

Глубокая нейронная сеть потенциально может аппроксимировать очень сложные функции при условии достаточно большого числа нейронов, а значит, и параметров. Чем меньше параметров, тем проще должна быть форма функции, чтобы наша сеть смогла ее аппроксимировать. Итак, правило 1: если потери на обучающем наборе данных не уменьшаются, вероятно, модель слишком проста \emph{для имеющихся данных}.

Что ж, если вычисленная на проверочном наборе данных функция потерь не убывает вместе с обучающим набором, значит, наша модель обучается лучше аппроксимировать полученные во время обучения примеры данных, но не \emph{обобщается} на примеры данных, которые не входят в этот конкретный набор. Правило 2: если потери на обучающем и проверочном наборах данных расходятся -- модель переобучена.

С интуитвной точки зрения более простая модель может описывать обучающие данные не так хорошо, как более сложная, но зато, вероятно, будет вести себя более равномерым образом между точами данных.

Следовательно, процесс выбора правильного размера нейросетевой модели в смысле количества параметров основан на двух шагах:
\begin{itemize}
	\item увеличение размера до тех пор, пока модель не будет хорошо подогнана к данным,
	
	\item а затем уменьшение, пока не будет устранено переобучение.
\end{itemize}

Разбиение набора данных. Перетасовка элементов тензора эквивалентна перестановке его индексов -- как раз то, что делает функция \verb|randperm|
\begin{lstlisting}[
style = bash,
numbers = none
]
n_samples = t_u.shape[0]
n_val = int(0.2 * n_samples)

shuffled_indicies = torch.randperm(n_samples)
train_indicies = shuffled_indicies[:-n_val]
test_indicies = shuffled_indicies[-n_val:]

train_t_u = t_u[train_indices]
train_t_c = t_c[train_indices]

val_t_u = t_u[val_indicies]
val_t_c = t_c[val_indicies]
\end{lstlisting}

Осталось только дополнительно вычислять потери на проверочном наборе данных \emph{на каждой эпохе}, чтобы заметить переобучение
\begin{lstlisting}[
style = bash,
numbers = none
]
def training_loop(n_epochs, optimizer, params, train_t_u, val_t_u, train_t_c, val_t_c):
    for epoch in range(1, n_epochs + 1):
        train_t_p = model(train_t_u, *params)
        train_loss = loss_fn(train_t_p, train_t_c)
        
        val_t_p = model(val_t_u, *params)
        val_loss = loss_fn(val_t_p, val_t_c)
        
        optimizer.zero_grad()
        # здесь только train_loss.backward(), поскольку мы не хотим обучать 
        # модель на проверочном наборе данных
        train_loss.backward()
        optimizer.step()
        
        if epoch <= 3 or epoch % 500 == 0:
            print(...)
            
    return params
\end{lstlisting}

Запуск
\begin{lstlisting}[
style = bash,
numbers = none	
]
params = torch.tensor([1.0, 0.0], requires_grad=True)
learning_rate = 1e-2
optimizer = optim.SGD([params], lr=learning_rate)

training_loop(
    n_epochs = 3000,
    optimizer = optimizer,
    params = params,
    train_t_u = train_t_un,
    val_t_u = val_t_un,
    train_t_c = train_t_c,
    val_t_c = val_t_c,
)
\end{lstlisting}

Наша цель -- убедиться, что убывают как потери на обучающем наборе данных, \emph{так} и потери на проверочном \cite[\strbook{186}]{pytorch-2022}.

Вопрос: раз мы никогда не вызываем \verb|backward()| для \verb|val_loss|, зачем вообще формировать граф вычислений? Можно просто вызывать \verb|model| и \verb|loss_fn| как обычные функции, без отслеживания истории вычислений.
\begin{lstlisting}[
style = ironpython,
numbers = none
]
def training_loop(n_epochs, optimizer, params, train_t_u, val_t_u, train_t_c, val_t_c):
    for epoch in range(1, n_epochs + 1):
        train_t_p = model(train_t_u, *params)
        train_loss = loss_fn(train_t_p, train_t_c)
        
        with torch.no_grad():
            val_t_p = model(val_t_u, *params)
            val_loss = loss_fn(val_t_p, val_t_c)
            # Убеждаемся, что для нашего вывода аргумент requires_grad == False
            assert val_loss.requires_grad == False
        
        optimizer.zero_grad()
        train_loss.backward()
        optimizer.step()
\end{lstlisting}

Нейрон -- по сути представляет собой линейное преобразование входного сигнала (например, умножение входного сигнала на какое-либо число (\emph{вес}) и прибавление к нему константы \emph{смещения}) с последующим применением фиксированной нелинейной функции (\emph{функции активации}).

Функция активации играет две выжные роли \cite[\strbook{195}]{pytorch-2022}:
\begin{itemize}
	\item Во внутренних частях модели благодаря функции активации возможны различные наклоны графика выходного сигнала в разных значениях -- нечто, по определению \emph{недоступное для линейной функции}. Искусно сочетая эти по-разному наклоненные участки для различных выходных сигналов, нейронные сети могут аппроксимировать любые функции.
	
	\item На последнем слое сети она локализует выходные сигналы предыдущей линейной операции в заданном интервале.
\end{itemize}

Нейрон -- это просто линейная функция с последующей функцией активации. 

Функции активации по определению \cite[\strbook{199}]{pytorch-2022}:
\begin{itemize}
	\item \emph{нелинейны} -- сколько ни применяй преобразование вида $w \cdot x + b$ без функции активации, все равно полчится функция той же самой (аффинной линейной) формы. {\color{blue}\emph{Нелинейность} позволяет сети в целом аппроксимировать более сложные функции},
	
	\item \emph{дифференцируемы}, что дает возможность вычисления градиентов. Точечные разрывы (Hardtanh, ReLU etc.), допустимы.
\end{itemize}

В отсутствие этих характеристик сеть либо превратиться обратно в линейную модель, либо с трудом будет поддаваться обучению.

Для функций активации справедливо следующее:
\begin{itemize}
	\item Имеется по крайней мере один диапазон \emph{чувствительности}, внутри которого нетривиальные изменения входного сигнала приводят к соответствующим нетривиальным изменениям выходного. Необходимо для обучения.
	
	\item У многих из них есть также диапазон \emph{нечувствительности} (\emph{насыщения}), в котором изменения входного сигнала практически не приводят к изменениям выходного.
\end{itemize}

В целом получается механизм, обладающий большими возможностями: при получении на входе различных данных в сети, составленной из \emph{линейных} и \emph{активационных блоков}:
\begin{itemize}
	\item различные нейроны могут возвращать для одних входных сигналов результаты, относящиеся к различным диапазонам,
	
	\item соответствующие этим входным сигналам ошибки в основном влияют на нейроны, работающие в диапазоне чувствительности, а на остальные блоки процесс обучения практически не влияет.
\end{itemize}

В результате объединения множества линейных и активационных блоков параллельно и последовательно получается математический объект, способный аппроксимировать сложные функции. Различные сочетания нейронов реагируют в различных диапазонах на входные сигналы, причем параметры этих блоков можно довольно легко оптимизировать посредством градиентного спуска, поскольку процесс обучения напоминает обучение линейной функции, вплоть до момента насыщения выходного сигнала.

В PyTorch есть отдельный подмодуль, посвященный нейронным сетям, -- \verb|torch.nn|. Он включает <<кирпичики>>, необходимые для создания всех видов нейросетевых архитектур. В терминологии PyTorch эти <<кирпичики>> называются \emph{модулями} (в других фреймворках подобные стандартные блоки часто называются \emph{слоями} (layers)). Модуль PyTorch -- это класс Python, наследующий базовый класс \verb|nn.Module|. 

Подмодули должны быть атрибутами верхнего уровня, а не быть закопаны внутри экземпляров list или dict! В противном случае оптимизатор не сможет их найти (а значит, и их параметры). На случай, если модели потребуется список или ассоциативный массив подмодулей, в PyTorch есть классы \verb|nn.ModuleList| и \verb|nn.ModuleDict|. У \verb|nn.Module| есть подкласс \verb|nn.Linear|, применяющий ко входным сигналам аффинное преобразование.

У всех подклассов \verb|nn.Module| в PyTorch есть метод \verb|__call__|, позволяющий создавать экземпляры \verb|nn.Linear| и вызывать их как функции следующим образом
\begin{lstlisting}[
style = bash,
numbers = none
]
import torch.nn as nn

linear_model = nn.Linear(1, 1)
linear_model(t_un_val)
\end{lstlisting}

Вызов экземпляра \verb|nn.Module| с набором инструментов приводит к вызову метода \verb|forward| с теми же аргументами, который реализует \emph{прямой проход} вычислений, в то время как \verb|__call__| выполняет другие немаловажные операции до и после вызова \verb|forward|. {\color{red} Так что формально можно вызвать \verb|forward| напрямую, и он вернет тот же результат, что и \verb|__call__|, но делать это из пользовательского кода не рекомендуется}
\begin{lstlisting}[
style = bash,
numbers = none
]
y = model(x)  # Правильно!
y = model.forward(x)  # НЕправильно! Так делать не надо!!!
\end{lstlisting}

Конструктор \verb|nn.Linear| принимает три аргумента: число входных принаков, число выходных признаков и булево значение, указывающее, включает линейная модель смещение или нет
\begin{lstlisting}[
style = bash,
numbers = none
]
import torch.nn as nn

linear_model = mm.Linear(1, 1)
linear_model(t_un_val)
\end{lstlisting}

Все модули в \verb|nn| ориентированы на генерацию выходных сигналов сразу для батча из нескольких входных сигналов. Следовательно, если нам нужно выполнить \verb|nn.Linear| для десяти примеров данных, можно создать входной тензор размеров $B \times N_{\text{вх}}$, где $B$ -- размер батча, а $N_{\text{вх}}$ -- число входных признаков, и пропустить его один раз через модель.

Причины для организации данных по батчам многогранны. Одна из них -- желание полноценно загрузить вычислениями имеющиеся вычислительные ресурсы. В частности, GPU позволяют сильно распараллеливать вычисления, так что при одиночном входном сигнале для маленькой модели большинство вычислительных элементов будет простаивать.

Еще одно преимущество в том, что некоторые развитые модели способны использовать статистическую информацию по целому батчу, и эти статистические показатели будут точнее при большом размере батча.

\begin{lstlisting}[
style = bash,
numbers = none
]
linear_model = nn.Linear(1, 1)
optimizer = optim.SGD(
    linear_model.parameters(),
    lr=1e-02,
)
\end{lstlisting}

При вызове метода \verb|training_loss.backward()| в листьях графа вычислений накапливаются градиенты. При вызове \verb|opimizer.step()| программа проходит по всем объектам \verb|Parameter| и меняет их на соответствующую содержимому атрибута \verb|grad| долю. 

\begin{lstlisting}[
style = bash,
numbers = none
]
def training_loop(
    n_epochs,
    optimizer,
    model,
    loss_fn,
    t_u_train,
    t_u_val,
    t_c_train,
    t_c_val,
):
    for epoch in range(1, n_epochs + 1):
        t_p_train = model(t_u_train)
        loss_train = loss_fn(t_p_train, t_c_train)
        
        t_p_val = model(t_u_val)
        loss_val = loss_fn(t_p_val, t_c_val)
        
        # требуется обязательно обнулять градиент на каждой итерации
        optimizer.zero_grad() 
        # выполняем проход в обратном направлении и вычисляем градиенты
        loss_train.backward()
        # обновляем параметры модели
        optimizer.step()
\end{lstlisting}

Модуль \verb|nn| включает несколько распространенных функций потерь, одна из которых -- \verb|nn.MSELoss|.

Модуль \verb|nn| предоставляет удобный способ соединения модулей цепочкой с помощью контейнера \verb|nn.Sequential|
\begin{lstlisting}[
style = bash,
numbers = none
]
seq_model = nn.Sequential(
    nn.Linear(1, 13),
    nn.Tanh(),
    nn.Linear(13, 1)
)
\end{lstlisting}

Модель переходит от одного входного признака до 13 скрытых признаков, пропускает их через функцию активации \verb|Tanh| и, наконец, объединяет получившиеся 13 чисел в один выходной признак.

Названия модулей в \verb|Sequential| представляют собой просто порядковые номера модулей в списке аргументов. Что любопытно, \verb|Sequential| также принимает на входе \verb|OrderDict|, в котором можно указать название каждого из передаваемых \verb|Sequential| модулей
\begin{lstlisting}[
style = bash,
numbers = none
]
from collections import OrderDict

seq_model = nn.Sequential(OrderDict([
    ("hidden_linear", nn.Linear(1, 8)),
    ("hidden_activation", nn.Tanh()),
    ("output_linear", nn.Linear(8, 1))
]))

for name, param in seq_model.named_parameters():
    print(name, param.shape)
# output
hidden_linear.weight torch.Size([8, 1])
hidden_linear.bias torch.Size([8])
output_linear.weight torch.Size([1, 8])
output_linear.bias torch.Size([1])
\end{lstlisting}

Обращаться к конкретным объектам \verb|Parameter| можно путем указания подмодулей в качестве атрибутов
\begin{lstlisting}[
style = bash,
numbers = none	
]
seq_model.output_linear.bias
# output
Parameter containing:
tensor([0.1402], requires_grad=True)
\end{lstlisting}

Можно посмотреть \emph{градиенты параметра} \verb|weight| линейной части скрытого слоя. Запускаем цикл обучения для новой модели нейронной сети, после чего смотрим на получившиеся градиенты после последней эпохи
\begin{lstlisting}[
style = bash,
numbers = none
]
seq_model.hidden_linear.weight.grad
\end{lstlisting}

Функции активации, в дополнение к линейным преобразованиям, позволяют нейронным сетям аппроксимировать сильно нелинейные функции, оставляя их при этом достаточно простыми для оптимизации \cite[\strbook{215}]{pytorch-2022}.

Чтобы преобразовать изображение PIL в тензор PyTorch, можно воспользоваться модулем \verb|torchvision.transforms|. Есть преобразование \verb|ToTensor|, превращающее массивы NumPy и изображения PIL в тензоры. Оно также располагает измерения выходного тензора в порядке $C \times H \times W$ (каналы, высота, ширина).
\begin{lstlisting}[
style = bash,
numbers = none
]
from torchvision import transforms, datasets

cifar10_train = datasets.CIFAR10(data_path, train=True, download=True)
img, label = ciraf10_train[99]

to_tensor = transforms.ToTensor()
img_t = to_tensor(img)
img_t.shape  # torch.Size([3, 32, 32])
\end{lstlisting}

Изображение \verb|img| превратилось в тензор формы $3 \times 32 \times 32$, то есть в изображение размером $32 \times 32$ с тремя цветовыми каналами (RGB).

Это преобразование можно передать непосредственно в виде аргумента \verb|datasets.CIFAR10|
\begin{lstlisting}[
style = bash,
numbers = none
]
tensor_cifar10 = datasets.CIFAR10(
    data_path,
    train=True,
    download=False,
    transform=transforms.ToTensor(),
)
\end{lstlisting}

Рекомендуемая практика -- нормализовать набор данных до нулевого среднего значения и единичного стандартного отклонения по каждому из каналов. 

При выборе функций активации, линейных около нуля ($\pm 1$ или $\pm 2$), ограничение данных тем же диапазоном повышает вероятность \emph{ненулевых градиентов нейронов}, а значит, и \emph{ускоряет обучение}. Кроме того, нормализация каналов к одинаковому распределению гарантирует смешение и обновление информации из разных каналов (посредством градиентного спуска) с одинаковой скоростью обучения.

Чтобы обеспечить нулевое среднее и единичное стандартное отклонение по каждому из каналов, необходимо вычислить среднее значение и стандартное отклонение каждого из каналов набора данных и применить следующее преобразование \verb|v_n[c] = (v[c] - mean[c]) / stdev[c]|. 

Поскольку набор данных CIFAR-10 невелик, можно работать с ним полностью в оперативной памяти. Разместим все возвращаемые объектом \verb|Dataset| тензоры последовательно в дополнительном измерении
\begin{lstlisting}[
style = bash,
numbers = none
]
imgs = torch.stack([img_t for img_t, _ in tensor_cifar_10], dim=3)
imgs.shape  # torch.Size([3, 32, 32, 50 000])
\end{lstlisting}

Теперь можно легко вычислить поканальные средние значения
\begin{lstlisting}[
style = bash,
numbers = none
]
imgs.view(3, -1)  # torch.Size([3, 51200000])
imgs.view(3, -1).mean(dim=1)
imgs.view(3, -1).std(dim=1)
\end{lstlisting}

Теперь
\begin{lstlisting}[
style = bash,
numbers = none
]
transformed_cifar10 = datasets.CIFAR10(
    data_path,
    train=True,
    download=False,
    transform=transforms.Compose([
        transforms.ToTensor(),
        transforsm.Normalize(
            mean=imgs.view(3, -1).mean(dim=1),
            std=imgs.view(3, -1).std(dim=1),
        )
    ])
)
\end{lstlisting}

Сколько признаков содержит каждый пример данных? Так, $3 \times 32 \times 32$ равняется 3072 входных признака на каждый пример. Получаем новую модель \verb|nn.Linear| с 3072 входными признаками и некоторым количеством скрытых принаков, за которым следует функция активации, а затем еще один \verb|nn.Linear|, сокращающий модель до соответствующего количества выходных признаков (в данном случае 2)
\begin{lstlisting}[
style = ironpython,
numbers = none
]
import torch.nn as nn

n_out = 2
model = nn.Sequential(
    nn.Linear(
        3072,  # входные признаки
        512,  # размер скрытого слоя
    ),
    nn.Tanh(),
    nn.Linear(
        512,  # размер скрытого слоя
        n_out,  # выходные признаки
    )
)
\end{lstlisting}

Нейронной сети требуется по крайней мере один скрытый слой (активации, поэтому два модуля) с \emph{нелинейностью} между слоями, чтобы сеть \emph{могла усваивать произвольные функции}, в противном случае модуль будет просто \emph{линейной}.

Здесь необходимо понять, что выходной сигнал носит категориальный характер: птица или самолет. Для представления категориальной величины следует воспользоваться унитарным кодированием, например, \verb|[1, 0]| для самолета и \verb|[0, 1]| для птицы (порядок выбран произвольно). Такая схема подходи и в случае 10 классов, как в полном наборе данных CIFAR-10; просто вектор будет длиной 10.

В нашем частном случае бинарной классификации (птица или самолет; по сути птица или не птица) два значения -- избыточно, поскольку одно всегда равно 1 минус второе. И действительно, PyTorch позволяет выдавать на выходе одно значение \emph{вероятности}, получая вероятность путем использования в конце модели функции активации \verb|nn.Sigmoid| и \underline{функции потерь} на основе \emph{бинарной перекрестной энтропии} \verb|nn.BCELoss|. Существует также \verb|nn.BCELossWithLogits|, объединяющая эти два шага \cite[\strbook{229}]{pytorch-2022}.

В идеальном случае сеть должна выдавать на выходе \verb|torch.tensor([1.0, 0.0])| для самолета и \verb|torch.tensor([0.0, 1.0])| -- для птицы. На практике же, поскольку наш класификатор не будет идеален, следует ожидать от сети неких промежуточных значений. Главное в этом случае, что мы можем интерпретировать выходные сигналы как вероятности: первая запись -- вероятность класса \verb|"airplane"|, а вторая -- \verb|"bird"|.

Чтобы развернуть изображение формы $3 \times 32 \times 32$, а затем преобразовать его в вектор-строку делаем так
\begin{lstlisting}[
style = bash,
numbers = none
]
img_batch = img.view(-1).unsqueeze(0)
img_batch.shape  # torch.Size([1, 3072]); 3 * 32 * 32 = 3072
\end{lstlisting}

Что касается функции потерь для задач классификации, то здесь необходимо накладывать штраф на ошибки классификации, а не кропотливо штрафовать все, что не равно в точности 0.0 или 1.0, поэтому в задачах классификации плохо работает среднеквадратическая функция потерь.

В этом случае \emph{необходимо максимизировать вероятность}, соответсвующую истинному классу.

Другими словами, нам нужна функция потерь, принимающая очень высокие значения, когда правдоподобие (истинность параметров нашей модели при имеющихся данных) низко: настолько низко, что вероятности альтернативных вариантов выше. И наоборот, потери должны быть низкими, когда правдоподобие данного варианта выше, чем у альтернатив, и мы не хотим зацикливаться на доведении вероятности до 1 \cite[\strbook{234}]{pytorch-2022}.

Действующая подобным образом функция потерь существует и называется \emph{отрицательной логарифмической функцией правдоподобия} (negative log likelihood, NNL). 

Для каждого примера данных в батче мы делаем следующее:
\begin{enumerate}
	\item Производим прямой проход и получаем выходные значения из последнего (линейного) слоя.
	
	\item Вычисляем для них многомерную логистическую функцию и получаем вероятности.
	
	\item Извлекаем предсказанную вероятность для истинного класса (правдоподобие параметров). Отметим, что истинный класс известен, поскольку обучение производится с учителем, -- это наши эталнные данные.
	
	\item Вычисляем ее логарифм, ставим перед ним знак <<минус>> и прибавляем к потерям.
\end{enumerate}

Функция \verb|nn.LogSoftmax()| обеспечивает численную устойчивость вычислений
\begin{lstlisting}[
style = bash,
numbers = none
]
model = nn.Sequential(
    nn.Linear(3072, 512),
    nn.Tanh(),
    nn.Linear(512, 2),
    nn.LogSoftmax(dim=1),
)
\end{lstlisting}

Среднеквадратическая функция потерь (MSE) насыщается намного раньше и -- что принципиально -- для совершенно неправильных предсказаний. Основная причина состоит в том, что уклон MSE слишком мал, чтобы компенсировать пологость многомерной логистической функции активации для неправильных предстказаний. {\color{red}Поэтому MSE для вероятностей плохо подходит для задач классификации} \cite[\strbook{237}]{pytorch-2022}.

\begin{lstlisting}[
style = bash,
numbers = none
]
import torch
import torch.nn as nn

model = nn.Sequential(
    nn.Linear(3072, 512),
    nn.Tanh(),
    nn.Linear(512, 2),
    nn.LogSoftmax(dim=1),
)

learning_rate = 1e-2

optimizer = optim.SGD(model.parameters(), lr=learning_rate)

n_epochs = 100

for epoch in n_epochs:
    for img, label in cifar2:
        out = model(img.view(-1).unsqueeze(0))
        loss = loss_fn(out, torch.tensor([label]))
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
   
   print(...)
\end{lstlisting}

Мы поняли, что обработка всех 10 000 изображений одним батчем -- это перебор, так что решили создать внутренний цикл, чтобы \emph{обрабатывать по одному примеру данных за раз} и производить \emph{обратное распространение ошибки по этому одному примеру}.

\emph{Перетасовывая} примеры данных \emph{на каждой эпохе} и вычисляя градиент по одному или (что желательно из соображений устойчивости) нескольким примерам данных за раз, мы фактически вносим элемент случайности в алгоритм градиентного спуска. Оказывается, что следование \emph{градиентам, вычисленным по мини-батчам}, которые представляют собой лишь слабые аппроксимации градиентов, вычисленных по всему набору данных, \emph{улучшает сходимость} и \emph{предотвращает <<застревание>>} процесса оптимизации во встреченных по пути локальных минимумах \cite[\strbook{238}]{pytorch-2022}.

Обычно размер мини-батча представляет собой константу, задаваему до обучения, аналогично скорости обучения. 

Модуль \verb|torch.utils.data| включает класс, помогающий с перетасовкой и организацией данных по мини-батчам: \verb|Dataloader|. Задача загрузчика данных состоит в выборе мини-батчей из набора данных с гибкими возможностями использования различных стратегий выборки. Одна из самых распространенных стратегий: равномерная выборка после \emph{перетасовки} данных \emph{в каждой эпохе}.

\begin{lstlisting}[
style = bash,
numbers = none
]
train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)
\end{lstlisting}

Пример
\begin{lstlisting}[
style = bash,
numbers = none
]
import torch
import torch.nn as nn

train_loader = torch.utils.data.DataLoader(
    cifar2, batch_size=64,
    shuffle=True,
)

model = nn.Sequential(
    nn.Linear(3072, 512),
    nn.Tanh(),
    nn.Linear(512, 2)
    nn.LogSoftmax(dim=1)
)

learning_rate = 1e-2

optimiizer = optim.SGD(model.parameters(), lr=learning_rate)

# отрицательный логарифм правдоподобия
loss_fn = nn.NLLLoss()

n_epochs = 100

for epoch in range(n_epochs):
    for imgs, labels in train_loader:
        batch_size = imgs.shape[0]
        outputs = model(imgs.view(batch_size, -1))
        loss = loss_fn(outputs, labels)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
\end{lstlisting}

На каждой итерации внутреннго цикла \verb|imgs| представляет собой мини-батч 64 RGB-изображений (размером $32 \times 32$), а \verb|labels| -- тензор размером 64 с индексами меток.

Наша цель в том, чтобы правильно присвоить изображениям метки классов, причем желательно на независимом наборе данных
\begin{lstlisting}[
style = bash,
numbers = none
]
val_loader = torch.utils.data.DataLoader(
    cifar2_val,
    batch_size=64,
    shuffle=False,  # NB
)

correct = 0
total = 0

with torch.no_grad():  # NB!
    for imgs, labels in val_loader:
        batch_size = imgs.shape[0]
        outputs = model(imgs.view(batch_size, -1))
        _, predicted = torch.max(outputs, dim=1)
        total += labels.shape[0]
        correct += int((predicted == labels).sum())

print(correct / total)
\end{lstlisting}

Достаточно часто последний слой \verb|nn.LogSoftmax| не включают в сеть, используя в качестве \emph{функции потерь} \verb|nn.CrossEntoryLoss|.

Полносвязная сеть не является \emph{инвариантной относительно сдвига}. Это значит, что сеть, обученная распознавать <<Спитфайер>> (самолет), начинающийся с позиции 4,4, не сможет распознать \underline{\itshape тот же самый} <<Спитфайен>>, начинающийся с позиции 8,8. Нам пришлось бы дополнять (augment) набор данных, то есть применять случайные сдвиги к изображениям во время обучения, чтобы сеть могла заметить <<Спитфайер>> в любом месте изображения, причем это пришлось бы делать для всех изображений в наборе \cite[\strbook{246}]{pytorch-2022}.

Учтите, что \emph{весовые коэффициенты ядра заранее не известны}, а инициализируются \emph{случайным} образом и обновляются посредством обратного распространения ошибки. Отметим также, что для всего изображения используется одно и то же ядро, а это значит, что и весовые коэффициенты ядра \cite[\strbook{252}]{pytorch-2022}. В производную функции потерь по свертрочным весам вности свой вклад все изображение.

Свертка эквивалентна нескольким линейным операциям, весовые коэффициенты которых равны нулю практически везде, кроме окрестностей отдельных пикселей, и получают одинаковые обновления во время обучения.

Очень часто применяют ядра, размеры которых одинаковы по всем измерениям, поэтому в PyTorch есть сокращенная форма записи для них: \verb|kernel_size=3| для двумерной свертки означает форму $3 \times 3$, для трехмерной свертки -- форму $3 \times 3 \times 3$.
\begin{lstlisting}[
style = bash,
numbers = none
]
conv = nn.Conv2d(3, 16, kernel_size=3)
# или так
conv = nn.Conv2d(3, 16, kernel_size=(3, 3))
\end{lstlisting}

Здесь 3 признака на один пиксель (3 канала; RGB) и 16 выходных каналов.

В результате прохода двумерной свертки получается двумерное изображение, пиксели которого представляют собой взвешенную сумму значений по локальным окрестностям входного изображения.

Как начальные значения весовых коэффициентов ядра \verb|conv.weight|, так и смещения задаются случайным образом, так что выходное изображение особого смысла не имеет.

Лучше предерживаться ядер нечентных размеров; ядра четного размера встречаются редко \cite[\strbook{256}]{pytorch-2022}.

Задача сверточной сети состоит в оценке ядра набора фильтров в последовательных слоях, преобразующих многоканальное изображение в другое многоканальное изображение, в котором различные каналы соответствуют разным признакам (например, один канал -- для среднего значения, другой -- для вертикальных краев и т.д.).

Первый набор ядер работает с маленькими окрестностями низкоуровневых признаков первого порядка, а второй набор фактически работает с более широкими окрестностями, \emph{генерируя признаки, представляющие собой композицию предыдущих признаков}. Благодаря этому замечательному механизму сверточные нейронные сети способны анализировать очень сложные кадры.

\begin{lstlisting}[
style = bash,
numbers = none
]
# свертка + нелинейность + пулинг
model = nn.Sequential(
    nn.Conv2d(3, 16, kernel_size=3, padding=1),  # свертка
    nn.Tanh(),  # нелинейность
    nn.MaxPool2d(2),  # пулинг
    nn.Conv2d(16, 8, kernel_size=3, padding=1),
    nn.Tanh(),
    nn.MaxPool2d(2),
    ...
)
\end{lstlisting}

Первая операция свертки превращает три канала RGB в 16, благодаря чему у сети появляется возможность генерировать \emph{16 независимых признаков} (16 каналов). Далее мы применяем функцию активации \verb|Tanh|. Полученное 16-канальное изображение $32 \times 32$ субдискритизируется первым слоем \verb|nn.MaxPool2d| до 16-канального изображения $16 \times 16$. 

Теперь субдискретизированное изображение подвергается еще одной операции свертки, выдающей на выходе 8-канальный выходной сигнал $16 \times 16$. Если повезет, это выходное изображение будет состоять из высокоуровневых признаков. И опять же мы применяем функцию активации \verb|Tanh|, после чего производим субдискретизацию до 8-канального выходного изображения $8 \times 8 $.

После уменьшения входного изображения до набора $8 \times 8$ признаков можно надеяться вернуть из сети значения вероятностей, подходящих для подачи на вход отрицательной логарифмической функции правдоподобия. Нужно преобразовать 8-канальное изображение $8 \times 8$ в одномерный вектор и завершить нашу сеть набором полносвязаных слоев
\begin{lstlisting}[
style = bash,
numbers = none
]
model = nn.Sequential(
    nn,Conv2d(3, 16, kernel_size=3, padding=1),
    nn.Tanh(),
    nn.MaxPool2d(2),
    nn.Conv2d(16, 8, kernel_size=3, padding=1),
    nn.Tanh(),
    nn.MaxPool2d(2),
    ...  # Пропущен важный момент
    nn.Linear(8 * 8 * 8, 32),
    nn.Tanh(),
    nn.Linear(32, 2),
)
\end{lstlisting}

Для \underline{повышения} \emph{разрешающих возможностей модели} можно \emph{увеличить количество выходных каналов} сверточных слоев (то есть число \emph{признаков}, генерируемых каждым из сверточных слоев), в результате чего увеличится и размер линейного слоя.

PyTorch позволяет производить в модели любые вычисления путем создания подклассов \verb|nn.Module|.

Готовые или пользовательские свертки как подмодули обычно включаются в программу посредством описания в конструкторе \verb|__init__| и присваивания их \verb|self| для использования в функции \verb|forward|. Их параметры в то же время храняться в них на протяжении всего жизненного цикла нашего модуля. Обратите внимание, что перед этим необходимо вызывать \verb|super().__init__()|.

\section{Обобщения с помощью сверток}

\subsection{Сеть как подкласс \texttt{nn.Module}}

Задача классификационных сетей обычно заключается в сжатии информации в том смысле, что мы начинаем с изображения, содержащего значительное количество пикселей, и сжимаем его в вектор вероятностей классов.

Для создания подкласса \verb|nn.Module| как минимум необходимо описать функцию \verb|forward|, принимающую входные сигналы модуля и возвращающую выходной.

Подмодули должны быть атрибутами верхнего уровня, а не быть <<закопаны>> внутри экземпляров list или dict! В противном случае оптимизатор не сможет их (а значит, и их параметры) найти. На случай, если модели потребуется список или ассоциативный массив подмодулей, в PyTorch есть классы \verb|nn.ModuleList| и \verb|nn.ModuleDict| \cite[\strbook{269}]{pytorch-2022}. 

\subsection{Функциональные API}

В PyTorch есть \emph{функциональные} аналоги для всех модулей \verb|nn|. Под функциональными здесь подразумевается <<без внутреннего состояния>> -- другими словами, <<выходное значение которых целиком и полностью определяется значениями входных аргументов>>. 

\begin{lstlisting}[
style = ironpython,
numbers = none
]
import torch.nn.functional as F

class Net(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)
        self.fc1 = nn.Linear(8 * 8 * 8, 32)
        self.fc2 = nn.Linear(32, 2)
        
   def forward(self, x):
       out = F.max_pool2d(  # пулинг
           torch.tanh(  # нелинейность
               self.conv1(x)  # свертка
           ), 2)
       out = F.max_pool2d(
           torch.tanh(
               self.conv2(x)
           ), 2)
       out = out.view(-1, 8 * 8 * 8)
       out = torch.tanh(self.fc1(out))
       out = self.fc2(out)
       
       return out
\end{lstlisting}

\subsection{Обучение модели}

В основе сверточной сети лежат два вложенных цикла: внешний -- по \emph{эпохам}, а внутренний -- на основе объекта \verb|DataLoader|, генерирующего батчи из объекта \verb|Dataset|. На каждой итерации цикла необходимо \cite[\strbook{273}]{pytorch-2022}:
\begin{itemize}
	\item Пропустить входные сигналы через модель (прямой проход).
	
	\item Вычислить функцию потерь (также часть прямого прохода).
	
	\item Обнулить все старые градиенты.
	
	\item Вызвать \verb|loss.backward()| для вычисления градиентов функции потерь относительно каждого из параметров (обратный проход).
	
	\item Оптимизировать в сторону уменьшения потерь.
\end{itemize}

\begin{lstlisting}[
style = bash,
numbers = none
]
import datetime

def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):
    for epoch in range(1, n_epochs + 1):
        loss_train = 0.0
        for imgs, labels in train_loader:
            outputs = model(imgs)
            loss = loss_fn(outputs, labels)
            
            # избавляемся от градиентов с предыдущих итераций
            optimizer.zero_grad()
            # выполняем обратный проход; то есть вычисляем градиенты
            # по всем обучаемым параметрам сети
            loss.backward()
            # обновляем модель
            optimizer.step()
            # суммируем потери за этоху
            loss_train += loss.item()
            
     if epoch == 1 or epoch % 10 == 0:
         # получаем средние потери на батч
         print("{} Epoch {}, Training loss {}".format(
             datetime.datetime.now(), epoch,
             loss_train / len(train_loader))
         )
\end{lstlisting}

Обучение в течение 100 эпох
\begin{lstlisting}[
style = bash,
numbers = none
]
# Объект DataLoader организует примеры данных из нашего набора по батчам.
# Перетасовка обеспечивает случайный порядок примеров данных из набора
train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=False)

model = Net()
optimizer = optim.SGD(model.parameters(), lr=1e-02)
loss_fn = nn.CrossEntropyLoss()

training_loop(
    n_epochs=100,
    optimizer=optimizer,
    model=model,
    loss_fn=loss_fn,
    train_loader=train_loader,
)
\end{lstlisting}

Сохранить модель можно так
\begin{lstlisting}[
style = bash,
numbers = none
]
# сохраняютс только веса
torch.save(model.state_dict(), data_path + "birds_vs_airplanes.pt")
\end{lstlisting}

Файл \verb|birds_vs_airplanes.pt| теперь содержит все параметры объекта \verb|model|: весовые коэффициенты и смещения для двух модулей свертки и двух линейных модулей. Никакой структуры, только весовые коэффициенты. Это значит, что при развертывании модели в реальных условиях нам понадобится описание класса \verb|model|
\begin{lstlisting}[
style = bash,
numbers = none	
]
loaded_model = Net()
loaded_model.load_state_dict(torch.load(data_path + "birds_vs_airplanes.pt))
\end{lstlisting}

В \verb|nn.Module| есть реализована функция \verb|.to|, перемещающая все параметры в GPU (или приводящая тип данных, если передать ей аргумент \verb|dtype|). 

Между \verb|Module.to| и \verb|Tensor.to| существует тонкое различие. \verb|Module.to| производит операции с заменой на месте, то есть изменяет экземпляр модуля. А \verb|Tensor.to| -- нет, возвращая \emph{новый тензор}.

Рекомендуемой практикой является создание экземпляра \verb|Optimizer| \emph{после} перемещения всех параметров на нужное устройство \cite[\strbook{275}]{pytorch-2022}.

Перенос вычислений на GPU при его наличии считается хорошим стилем программирования. Неплохим паттерном программирования будет установка значения переменной \verb|device| в зависимости от \verb|torch.cuda.is_avaible|:
\begin{lstlisting}[
style = bash,
numbers = none
]
device = (
    torch.device("cuda") if torch.cuda.is_avaible() else torch.device("cpu")
)
\end{lstlisting}

\begin{lstlisting}[
style = bash,
numbers = none
]
def training_loop(
    n_epochs, optimizer,
    model, loss_fn, train_loader
):
    for epoch in range(1, n_epochs + 1):
        loss_train = 0.0
        for imgs, labels in train_loader:
            imgs = imgs.to(device=device)  # NB
            labels = labels.to(device=device)  # NB
            outputs = model(imgs)
            loss = loss_fn(outputs, labels)
\end{lstlisting}

Можно создать экземпляр модели и перенести ее на \verb|device|
\begin{lstlisting}[
style = bash,
numbers = none
]
train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)

# перенести модель (все ее параметры) на GPU
model = Net().to(device=device)
optimizer = optim.SGD(model.parameters(), lr=1e-02)
loss_fn = nn.CrossEntropyLoss()

training_loop(
    n_epochs=100,
    optimizer=optimizer,
    model=model,
    loss_fn=loss_fn,
    train_loader=train_loader,
)
\end{lstlisting}

Если забыть перенести саму модель или входные данные на GPU, вы получите сообщения об ошибках, указывающие, что тензоры располагаются на различных устройствах, поскольку операторы PyTorch не поддерживают смеси входных данных на GPU и CPU.

PyTorch попытается загрузить веса на то же устройство, с которого они были сохранены, то есть весовые коэффициенты с GPU будут восстановлены на GPU. Лаконичным вариантом будет потребовать от PyTorch переопределить информацию об устройстве при загрузке весовых коэффициентов
\begin{lstlisting}[
style = bash,
numbers = none
]
loaded_model = Net().to(device=device)
loaded_model.load_state_dict(torch.load(data_path + "birds_vs_airplanes.pt", map_location=device))
\end{lstlisting}







% Источники в "Газовой промышленности" нумеруются по мере упоминания 
\begin{thebibliography}{99}\addcontentsline{toc}{section}{Список литературы}
	
	\bibitem{pytorch-2022}{\emph{Стивенс Э.} PyTorch. Освещая глубокое обучение. -- СПб.: Птер, 2022. -- 576 с.}
\end{thebibliography}

%\listoffigures\addcontentsline{toc}{section}{Список иллюстраций}

%\lstlistoflistings\addcontentsline{toc}{section}{Список листингов}

\end{document}
