\documentclass[%
	11pt,
	a4paper,
	utf8,
	%twocolumn
		]{article}	

\usepackage{style_packages/podvoyskiy_article_extended}


\begin{document}
\title{Приемы работы с библиотекой PyTorch}

\author{}

\date{}
\maketitle

\thispagestyle{fancy}

\tableofcontents

\section{Вводные замечания}

Чаще всего цикл обучения модели реализуют в виде обычного цикла \verb|for| Python. Оптимизатор, доступный в модуле \verb|torch.optim| PyTorch, который будет отвечать за обновление параметров. 

По умолчанию в PyTorch используется модель немедленного выполнения (eager mode). Как только интерпретатор Python выполняет инструкцию, связанную с PyTorch, базовая реализация C++ или CUDA сразу же производит соответствующую операцию.

PyTorch также предоставляет возможности предварительной компиляции моделей с помощью TorchScript. Используя TorchScript, PyTorch может преобразовать модель в набор инструкций, которые можнро независимо вызывать из Python, допустим, из программ на C++ или на мобильных устройствах. Это можно считать своего рода виртуальной машиной с ограниченным набором инструкций, предназначенным для опреаций с тензорами. Экспортировать модель можно либо в виде TorchScript для использования со средой выполнения Python, либо в стандартизированном формате ONNX (платформонезависимый формат описания моделей).

Сети среднего размера могут потребовать от нескольких часов до нескольких дней для обучения с нуля на больших реальных наборах данных на рабочих станциях с хорошим GPU \cite[\strbook{48}]{pytorch-2022}. Длительность обучения можно сократить за счет использования на одной машине нескольких GPU или даже еще сильнее -- на кластере машин, оснащенных несколькими GPU. 

Для примера создадим сеть AlexNet
\begin{lstlisting}[
style = ironpython,
numbers = none	
]
# TorchVision включает несколько лучших нейросетевых архитектур для машинного зрения
from torchvision import models

alexnet = models.AlexNet()
\end{lstlisting}

Подав на вход \verb|alexnet| данные четко определенного размера, мы выполним прямой проход (forward pass) по сети, при котором входной сигнал пройдет через первый набор нейронов, выходные сигналы которых будут поданы на вход следующего набора нейронов, и так до самого итогового выходного сигнала. На практике это означает, что при наличии объекта \verb|input| нужного типа можно произвести прямой проход с помощью оператора \verb|ouput = alexnet(input)|.

Но если мы так поступим, то получим мусор. А все потому, что сеть не была инициализрована: ее веса, числа, с которыми складываются и на которые умножаются входные сигналы, не были обучены на чем-либо, сеть сама по себе -- чистый (или, точнее, сказать случайный) лист. Необходимо либо обучить ее с нуля, либо загрузить веса, полученные в результате предыдущего обучения \cite[\strbook{58}]{pytorch-2022}.

В \verb|models| названия в верхнем регистре соответствуют классам, реализующим популярные архитектуры, предназначенные для машинного зрения. С другой стороны, названия в нижнем регистре соответствуют функциям, создающим экземпляры моделей с заранее определенным количеством слоев и нейронов, а также, возможно, скачивающие и загружающие в них предобученные веса.

Для того чтобы привести входные изображения к нужному размеру, а их значения (цвета) примерно в один числовой диапазон, можно воспользоваться преобразованиями модуля \verb|torchvision|
\begin{lstlisting}[
style = ironpython,
numbers =  none
]
from torchvision import transforms

# это функция
preprocess = transformers.Compose([
    transformers.Resize(256),
    transformers.CenterCrop(224),
    transformers.ToTensor(),
    transformers.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    )
])
\end{lstlisting}

Здесь описана функция \verb|preprocess|, масштабирующую входное изображение до размера $256 \times 256$, обрезающую его до $224 \times 224$ по центру, преобразующую в тензор (многомерный массив PyTorch: в данном случае трехмерных массив, содержащий цвет, высоту и ширину) и нормализующую его компоненты RGB (красный, зеленый, синий) до заданных среднего значения и стандартного отклонения.

Если мы хотим получить от сети осмысленные ответы, все это должно соответствовать данным, полученным сетью во время обучения.

Процесс выполнения обученной модели на новых данных в сфере глубокого обучения называется \emph{выводом} (inference). Для выполнения вывода необходимо перевести сеть в режим \verb|eval|
\begin{lstlisting}[
style = ironpython,
numbers = none
]
resnet.eval()
\end{lstlisting}

Если забыть сделать это, некоторые предоубченные модели, например включающие нормализацию по мини-батчам и дропаут, не дадут никаких осмысленных результатов просто по причине их внутреннего устройства. Теперь, после установки режима \verb|eval|, можно выполнять вывод
\begin{lstlisting}[
style = ironpython,
numbers = none
]
out = resnet(batch_t)
# получается что-то вроде степени уверенности модели в конкретном предсказании
percentage = torch.nn.functional.softmax(out, dim=1)[0] * 100
\end{lstlisting}

Успешность работы сети во многом зависит от наличия соответствующих объектов в обучающем наборе данных. Если подать нейронной сети нечто выходящее за рамки обучающего набора данных, вполне возможно, что она достаточно уверенно вернет неправильный ответ \cite[\strbook{64}]{pytorch-2022}.

Сеть представляет собой всего лишь каркас, а вся суть -- в весовых коэффициентах \cite[\strbook{69}]{pytorch-2022}.

\subsection{Torch Hub}

Автору, чтобы опубликовать модель через механизм Torch Hub, необходимо всего лишь поместить файл \verb|hubconf.py| в корневой каталог репозитория GitHub. Структура очень проста
\begin{lstlisting}[
style = bash,
numbers = none
]
# необязательный список модулей, от которых зависит данный код
dependencies = ["torch", "math"]

# одна или несколько функций, открываемых пользователям в качестве входных точек репозитория. Эти функции должны инициализировать модели в соответствии с агрументами и возвращать их
def some_entry_fn(*args, **kwargs):
    model = build_some_model(*args, **kwargs)
    return model
    
def another_entry_fn(*args, **kwargs):
    model = build_another_model(*args, **kwargs)
    return model
\end{lstlisting}

Теперь интересные предобученные модели можно искать в репозиториях GitHub, содержащих файл \verb|hubconf.py|, зная сразу же, что их можно будет загрузить с помощью модуля torch.hub.
\begin{lstlisting}[
style = bash,
numbers = none	
]
import torch
from torch import hub

resnet18_model = hub.load(
    "pytorch/vision:main",  # название и ветка репозитория GitHub
    "resnet18",  # название точки входа
    pretrained=True  # ключевой аргумент
)
\end{lstlisting}

Приведенный код скачивает копию состояния ветки \verb|main| репозитория \verb|pytorch/vision|, вместе с весовыми коэффициентами в локальный каталог (по умолчанию \verb|.torch/hub| в домашнем каталоге) и выполняет функцию точки входа \verb|resnet18|, возвращающую созданный экземпляр модели.

\section{Тензор}

В контексте глубокого обучения тензоры связаны с обобщением векторов и матриц на произвольную размерность. Другими словами, речь идет о многомерных массивах.

По сравнению с массивами NumPy тензоры PyTorch обладают несколькими потрясающими способностями, например возвожностью чрезвычайно быстро выполнять операции на графических процессорах, умением распределять операции по нескольким устройствам или машинам, а также отслеживать породивший их граф вычислений. 

\emph{\color{blue}Тензоры PyTorch и массивы NumPy} это представления над (обычно) \emph{\color{blue}непрерывными блоками памяти}, содержащими распакованные (unboxed) числовые типы данных Си, \emph{а не объекты Python} \cite[\strbook{83}]{pytorch-2022}.

Пример тензора
\begin{lstlisting}[
style = bash,
numbers = none
]
img_t = torch.randn(3, 5, 5)
batch_t = torch.tensor(2, 3, 5, 5)  # [батч, каналы, строки, столбцы]
\end{lstlisting}

Иногда каналы RGB размещаются в измерении 0, а иногда -- в измерении 1. Но обобщение можно производить путем отсчета с конца: \emph{каналы} всегда расположены в измерении -3, третьем с конца. 
\begin{lstlisting}[
style = bash,
numbers = none
]
img_gray_naive = img_t.mean(-3)
batch_gray_naive = batch_t.mean(-3)
img_gray_naive.shape, batch_gray_naive.shape  # (torch.Size([5, 5]), torch.Size([2, 5, 5]))
\end{lstlisting}

PyTorch автоматически добавляет в начало измерение размером 1. Эта функция называется \emph{транслированием} (broadcasting). \verb|batch_t| формы $(2, 3, 5, 5)$ умножается на \verb|unsqueeze_weights| формы $(3, 1, 1)$, в результате чего получается тензор формы $(2, 3, 5, 5)$, в котором затем можно сложить третье измерение с конца (три канала).
\begin{lstlisting}[
style = bash,
numbers = none
]
weights = torch.tensor([0.2126, 0.7152, 0.0722])  # torch.Size([3])
unsqueezed_weights = weights.unsqueeze(-1).unsqueeze_(-1)  # torch.Size([3, 1, 1])
img_weights = (img_t * unsqueezed_weights)
batch_weights = (batch_t * unsqueezed_weights)
\end{lstlisting}

В PyTorch 1.3 добавилась экспериментальная возможность \emph{именновых тензоров}. У функций создания тензоров, например \verb|tensor| и \verb|rand|, есть аргумент \verb|names|. В качестве аргумента \verb|names| должна передаваться последовательность строковых значений
\begin{lstlisting}[
style = bash,
numbers = none
]
weights_named = torch.tensor([0.2126, 0.7152, 0.0722], names=["channels"])
\end{lstlisting}

При необходимости добавить названия в имеющийся тензор (не меняя существующие) можно вызвать его метод \verb|refine_names|. Аналогично доступу по индексу с помощью многоточия можно пропускать любое количество измерений. С помощью родственного ему метода \verb|rename| можно также переопределять или удалять (путем передачи \verb|None|) уже существующие названия
\begin{lstlisting}[
style = bash,
numbers = none
]
img_named = img_t.refine_names(..., "channels", "rows", "columns")
\end{lstlisting}

Метод \verb|align_as| возвращает тензор, в котором добавлены недостающие измерения, а уже существующие переставлены в нужном порядке \cite[\strbook{89}]{pytorch-2022}
\begin{lstlisting}[
style = bash,
numbers = none
]
weights_named.shape  # torch.Size([3])
img_named.shape  # torch.Size([3, 5, 5])
weights_aligned = weights_named.align_as(img_named)
weights_aligned.shape  # torch.Size([3, 1, 1])
\end{lstlisting}

Функции, принимающие на входе аргументы для измерений, также позволяют указывать поименованные измерения
\begin{lstlisting}[
style = bash,
numbers = none
]
(img_named * weights_aligned).sum("channels")
# то же самое
(img_named * weights_aligned).sum(0)
\end{lstlisting}

При попытке сочетать измерения с различными названиями выдается сообщение об ошибке
\begin{lstlisting}[
style = bash,
numbers = none
]
# img_named[..., :3] то же самое, что и img_named[:, :, :3]
gray_named = (img_named[..., :3] * weights_named).sum("channels")  # Ошибка, т.к. размерности не совпадают
# а так можно
(img_named[:, :, :3] * weights_named).sum("channels")
\end{lstlisting}

При необходимости использовать тензоры не только в функциях, работающих с поименованными тензорами, необходимо удалить названия, установив их в \verb|None|
\begin{lstlisting}[
style = bash,
numbers = none
]
img_named.rename(None)
\end{lstlisting}

\subsection{Типы элементов тензоров}

Использовать стандартные типы данных Python не рекомендуется по нескольким причинам \cite[\strbook{90}]{pytorch-2022}:
\begin{itemize}
	\item \emph{Числовые значения в Python являются \underline{объектами}}. В то время как число с плавающей запятой требует для представления в компьютере только 32 бита, Python преобразует его в \emph{\color{red}полноценный объект Python} с подсчетом ссылок и т.д. Эта операция, которая называется \emph{упаковой} (boxing), не является проблемой при хранении небольшого количества числовых значений, но выделять память для миллионов таких объектов -- совершенно нерационально.
	
	\item \emph{Списки в Python предназнены для хранения последовательных наборов \underline{объектов}}. В них нет операций для быстрого вычисления скалярного произведения двух векторов или их суммирования. Кроме того, \emph{списки Python} не оптимизируют размещение своего содержимого в памяти, поскольку представляют собой \emph{наборы указателей на объекты Python} (любые, не только числовые значения) с доступом по индексу. Наконец, списки Python одномерны, и, хотя можно создавать списки списков, это тоже нерационально.
	
	\item \emph{Интерпретатор Python работает медленно по сравнению с оптимизированным, скомпилированным кодом}.
\end{itemize}

Вычисления в нейронных сетях обычно производятся над 32-битными значениями с плавающей запятой. Более высокая точность, например 64-битные значения, обычно не повышает безошибочность модели, но требует больше памяти и вычислительного времени. Нативная поддержка типа данных с половинной точностью -- 16-битных значений с плавающей запятой -- в стандартных CPU обычно отсутствует, зато предоставляется современными GPU. При необходимости можно перейти на \emph{половинную точность} для снижения объема занимаемой памяти нейросетевой модели без особого влияния на степень безошибочности \cite[\strbook{92}]{pytorch-2022}.

Привести результат функции создания тензора к нужному типу с помощью соответствующего метода приведения типов, можно так
\begin{lstlisting}[
style = bash,
numbers = none
]
torch.zeros(10, 2).double()
torch.ones(10, 2).short()
\end{lstlisting}

Или с помощью более удобного метода \verb|.to()|
\begin{lstlisting}[
style = bash,
numbers = none	
]
torch.zeros(10, 2).to(torch.double)
torch.ones(10, 2).to(dtype=torch.short)
\end{lstlisting}

Память под значения в тензорах выделяется непрерывными фрагментами памяти под управлением экземпляров \verb|torch.Storage|. Хранилище представляет собой одномерный массив числовых данных, то есть непрерывный фрагмент памяти, содержащий числа заданного типа, например \verb|float| (32-битные значения, выражающие числа с плавающей запятой) или \verb|int64| (64-битные значения, выражющие целые числа). Экземпляр класса \verb|Tensor| PyTorch -- это представление подобного экземпляра \verb|Storage| с возможностью доступа к хранилищу по индексу через указание сдвига и шага по каждому измерению.

Хранилище \emph{всегда} представляет собой \emph{одномерный} массив вне зависимости от размерности каких-либо ссылающихся на него тензоров.

Методы, имена которых заканчиваются на символ <<\verb|_|>>, как в \verb|zero_|, указывают, что метод работает с заменой на месте (in place), изменяя входный данные вместо того, чтобы создавать новый выходной тензор и возвращать его. Метод \verb|zero_| обнуляет все элементы входного тензора. Все методы, в конце названия которых нет символа подчеркивания, оставляют исходный тензор неизменным и вместо этого возвращают новый.

Для транспонирования двумерных тензоров используется метод \verb|.t()|. Но в PyTorch транспорировать можно не только матрицы. Можно транспонировать многомерный массив, и для этого достаточно указать два измерения, по которым нужно произвести транспонирование (зеркально отражая форму шага)
\begin{lstlisting}[
style = bash,
numbers = none
]
somt_t = torch.ones(3, 4, 5)
transpose_t = some_t.transpose(0, 2)
some_t.shape  # torch.Size([3, 4, 5])
transpose_t.shape  # torch.Size([5, 4, 3])
\end{lstlisting}

Любой из тензоров PyTorch можно перенести на (один из) GPU системы для массово-параллельных быстрых вычислений.

Помимо \verb|dtype|, класс \verb|Tensor| предоставляет атрибут \verb|device|, который описывает, где на компьютере размещаются данные тензора. 
\begin{lstlisting}[
style = bash,
numbers = none
]
points_gpu = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]], device="cuda")
\end{lstlisting}

Вместо этого можно скопировать созданный в CPU тензор на GPU с помощью метода \verb|to|
\begin{lstlisting}[
style = bash,
numbers = none	
]
points_gpu = points.to(device="cuda")
\end{lstlisting}

При этом возвращается новый тензор с теми же числовыми данными, но хранящийся в \emph{памяти GPU}, {\color{red}а не в обычной \emph{оперативной памяти системы}} \cite[\strbook{105}]{pytorch-2022}.

Если на нашей машине более одного GPU, можно также указать, на каком именно GPU размещать тензор, передав отсчитываемый с нуля целочисленный номер GPU на машине, вот так
\begin{lstlisting}[
style = bash,
numbers = none	
]
points_gpu = points.to(device="cuda:0")

# умножение выполняется на CPU
points = 2 * points
# умножение выполняется на GPU
points_gpu = 2 * points.to(device="cuda")
\end{lstlisting}

Отметим, что тензор \verb|points_gpu| не передается обратно в CPU после вычисления результата. Вот что происходит в этой строке:
\begin{itemize}
	\item Тензор \verb|points| копируется в GPU.
	
	\item Выделяется память в GPU под новый тензор, в котором будет храниться результат умножения.
	
	\item Возвращается обращение к этому GPU-тензору.
\end{itemize}

Следовательно, если мы прибавим к результату константу
\begin{lstlisting}[
style = bash,
numbers = none
]
points_gpu = points_gpu + 4
\end{lstlisting}
операция сложения будет по-прежнему производиться в GPU и никакой информации в CPU передаваться не будет (если мы не будем выводить полученный тензор на экран или обращаться к нему). Для переноса тензора обратно в CPU необходимо указать в методе \verb|.to()| аргумент \verb|cpu|
\begin{lstlisting}[
style = bash,
numbers = none
]
points_gpu.to(device="cpu")
\end{lstlisting}

Можно также для получения того же результата воспользоваться сокращенными методами \verb|.cpu()| и \verb|.cuda()| вместо метода 
\begin{lstlisting}[
style = bash,
numbers = none
]
points_gpu = points.cuda()
points_gpu = points.cuda(0)
points_gpu.cpu()
\end{lstlisting}

Стоит упомянуть, что с помощью метода \verb|.to()| можно менять тип данных и их место размещения одновременно, указав в качестве аргументов \verb|device| и \verb|dtype|.

Тензоры PyTorch можно очень эффективно преобразовать в массивы NumPy и наоборот. Благодаря этому можно воспользоваться огромными объемами функциональности экосистемы Python, основанной на типах массивов NumPy. Подобная совместимость с массивами NumPy, не требующая копирования кода, возможна благодаря работы системы хранения с буферным протоколом Python.

В разреженных тензорах хранятся только ненулевые значения, а также информация об индексах.

Для сериализации обхектов-тензоров PyTorch использует <<за кулисами>> \verb|pickle|, а также специализированный код сериализации для хранилища. Вот как можно сохранить наш тензор \verb|points| в файл \verb|ourpoints.t|
\begin{lstlisting}[
style = bash,
numbers = none
]
torch.save(points, "./data/p1ch3/ourpoints.t)
\end{lstlisting}

Либо можно передать файловый дескриптор файла вместо названия
\begin{lstlisting}[
style = bash,
numbers = none
]
with open("./data/p1ch3/ourpoints.t", mode="wb") as f:
    torch.save(points, f)
\end{lstlisting}

Загрузка тензора \verb|points| обратно также выполняется одной строкой кода
\begin{lstlisting}[
style = bash,
numbers = none
]
points = torch.load("./data/p1ch3/ourpoints.t")
\end{lstlisting}
что эквивалентно
\begin{lstlisting}[
style = bash,
numbers = none
]
with open("./data/p1ch3/ourpoints.t", mode="rb") as f:
    points = torch.load(f)
\end{lstlisting}

И хотя подобным образом можно быстро сохранять тензоры, если нужно загрузить их только в PyTorch, сам по себе формат файла не отличается совместимостью: прочитать тензор с помощью какого-либо еще ПО, помимо PyTorch, не получится.

HDF5 -- переносимый, широко поддерживаемый формат представления сериализованных многомерных массивов, организованный в виде вложенного ассоциативного массива типа <<ключ--значение>>. Python поддерживает формат HDF5 благодаря библиотеке \verb|h5py|, принимающей и возвращающей данные в виде массивов NumPy.
\begin{lstlisting}[
style = bash,
numbers = none
]
import h5py

f = h5py.File("./ourpoints.hdf5", "w")
dset = f.create_dataset("coords", data=points.numpy())
f.close()
\end{lstlisting}

Здесь \verb|"coords"| -- это ключ для файла в формате HDF5. В HDF5 интересна возможность индексации набора данных на диске и обращения только к нужным нам элементам. 
\begin{lstlisting}[
style = bash,
numbers = none
]
f = h5py.File("./ourpoints.hdf5", "r")
dset = f["coords"]
torch.from_numpy(dset[-2:])
f.close()
\end{lstlisting}

\subsection{Представление реальных данных с помощью тензоров}

\subsubsection{Работа с изображениями}

Изображения представляются в виде набора скалярных значений расположенных на равномерной сетке с высотой и шириной (в пикселях), например, по одному скалярному значению на каждую точку сетки (пиксель) для изображения в оттенках серого или несколько скалярных значений на каждую точку сетки для представления различных цветов.

Отражающие значения для различных пикселей скаляры обычно кодируются 8-битными целыми числами, как в бытовых фотоаппаратах. В медицинских, научных и промышленных приложениях нередко встречается более высокая точность, например 12- или 16-битная, для расширения диапазона или повышения чувствительности в случаях, когда пиксель отражает информацию о физическом свойстве, например о плотности костной ткани, температуре или глубине.

Загрузить изображение можно так
\begin{lstlisting}[
style = bash,
numbers = none	
]
import imageio

img_arr = imageio.imread("./bobby.jpg")
img_arr.shape  # (720, 1280, 3)
\end{lstlisting}

Модули PyTorch, работающие с изображениями, требуют от тензоров измерений $C \times H \times W$ (каналы, высота и ширина).

Для получения нужной нам схемы расположения можно воспользоваться методом \verb|permute| тензора, указав в качестве параметров старые измерения для каждого из новых.
\begin{lstlisting}[
style = bash,
numbers = none	
]
img = torch.from_numpy(img_arr)
out = img.permute(2, 0, 1)
\end{lstlisting}

Эта операция \underline{не копирует} данные тензора, вместо этого \verb|out| \emph{использует то же самое хранилище}, что и \verb|img|, только меняя информацию о размере и шаге на уровне тензора.

Несколько более эффективная альтернатива использованию для создания тензора \verb|stack| -- выделить заранее память под тензор нужного размера, а затем заполнить его загруженными из каталога изображениями следующим образом
\begin{lstlisting}[
style = bash,
numbers = none
]
batch_size = 3
batch = torch.zeros(batch_size, 3, 256, 256, dtype=torch.uint8)
\end{lstlisting}

Батч будет состоять из трех RGB-изображений по 256 пикселей высотой и 256 пикселей шириной. 

Нейронные сети демонстрируют наилучшее качество обучения, когда входные данные находятся в диапазоне примерно от 0 до 1 или от -1 до 1.

Никаких принципиальных различий между тензорами, содержащими объемные пространственные данные и данные изображения, нет. Просто появляется дополнительное измерение, глубина, вслед за измерением каналов, и получается 5-мерный тензор формы $N \times C \times D \times H \times W$.

Загрузим пример КТ-снимка с помощью функции \verb|volread| из модуля \verb|imageio|, принимающий в качестве аргумента каталог и собирающей все файлы в формате DICOM (Digital Imaging and Communications in Medicine) в трехмерный массив NumPy
\begin{lstlisting}[
style = bash,
numbers = none
]
import imageio

dir_path = ".../2-LUNG 3.0B70f-04083"
vol_arr = imageo.volread(dir_path, "DICOM")
vol_arr.shape  # (99, 512, 512)

vol = torch.from_numpy(vol_arr).float()
vol = torch.unsqueeze(vol, 0)
vol.shape  # torch.Size([1, 99, 512, 512]): каналы, глубина, высота, ширина
\end{lstlisting}

При вызове метода \verb|.view()| на тензоре возвращает новый тензор с другими размерностью и шагами \emph{без изменения хранилища}. Это позволяет перегруппировать тензор прктически без затрат, поскольку \emph{никакие данные копировать не нужно}.

Нормализацию данных к отрезку $[0; 1]$ или $[-1; 1]$ желательно производить \emph{для всех количественных величин}, таких как <<температура>> (это полезно для процесса обучения) \cite[\strbook{137}]{pytorch-2022}.

По завершении обучения алгоритм способен генерировать правильные выходные сигналы при получении новых данных, \emph{достаточно схожих} со входными данными, на которых он обучался. В случае глубокого обучения этот процесс работает даже тогда, когда входные данные и требуемые выходные сгиналы \emph{далеки} друг от друга: когда они относятся к различным предметным областям.

У нас есть модель с неизвестными значениями параметров и нужно получить оценку этих параметров, которая бы минимизировала расхождение между предсказанными выходными сигналами и измеренными значениями (ошибка). Целью процесса оптимизации должен быть поиск таких параметров модели, которые минимизировали бы функцию потерь.

Отдельная итерация обучения, во время которой обновляются параметры для всех обучающих примеров данных, называется \emph{эпохой}.

Градиенты по параметрам модели должны быть одного порядка \cite[\strbook{168}]{pytorch-2022}.

Аргумент \verb|requires_grad| указывает PyTorch отслеживать целое семейство тензоров. Если функции дифференцируемые (как большинство операций над тензорами PyTorch), величина производной будет автоматически занесена в атрибут \verb|.grad|.

Количество тензоров с параметром \verb|requires_grad|, установленным в \verb|True| аргументом, и композиции функций может быть любым. В этом случае PyTorch вычисляет производные функции потерь по всей цепочке функций (графу вычислений) и накапливает их значения в атрибутах \verb|.grad| этих тензоров (узлы этого графа).

При вызове \verb|.backward()| производные \emph{накапливаются} в узлах-листьях. {\color{blue}Необходимо \emph{явным образом обнулять градиенты} после обновления параметров на их основе}. Так что, если \verb|backward| вызывался ранее, потери оцениваются опять, \verb|backward| вызывается снова, после чего накапливаются градиенты во всех листьях графа, то есть суммируются с вычисленными на предыдущей итерации, в результате чего неправильное значение градиента \cite[\strbook{173}]{pytorch-2022}.

Чтобы предотвратить подобное, необходимо \emph{явным образом обнулять градиенты} на каждой итерации.

\begin{lstlisting}[
style = bash,
numbers = none
]
def training_loop(n_epochs, learning_rate, params, t_u, t_c):
    for epoch in range(1, n_epochs + 1):
        if params.grad. is not None:
            params.grad.zero_()
            
        t_p = model(t_u, *params)
        loss = loss_fn(t_p, t_c)
        loss.backward()  # выполяем обратный проход и вычисляем градиенты
        
        with torch.no_grad():
            params -= learning_rate * params.grad
            
        if epoch % 500 == 0:
            print(...)
\end{lstlisting}

При вызове \verb|loss.backward()| PyTorch обходит граф в обратном порядке, вычисляя градиенты. Контекст \verb|torch.no_grad()| означает, что механизм автоматического вычисления градиента игнорирует внутренности блока \verb|with|: то есть не добавляет ребра в граф прямого прохода.

У каждого оптимизатора доступны два метода: \verb|zero_grad| и \verb|step|. Метод \verb|zero_grad| обнуляет атрибут \verb|grad| всех передаваемых оптимизатору параметров при его создании. А метод \verb|step| обновляет значения параметров в соответствии с реализуемой конкретным оптимизатором стратегией оптимизации.
\begin{lstlisting}[
style = bash,
numbers = none	
]
def training_loop(n_epochs, optimizer, params, t_u, t_c):
    for epoch in range(1, n_epochs + 1):
        t_p = model(t_u, *params)
        loss = loss_fn(t_p, t_c)
        
        # обнуляем градиент, потому как в противном случае
        # производные накапливались бы в узлах-листьях графа вычислений
        optimizer.zero_grad()
        loss.backward()  # обратный проход по графу; вычисляем градиенты
        optimizer.step()  # обновляем параметры модели
        
        if epoch % 500 == 0:
            print(...)
            
    return params
\end{lstlisting}

Очень гибкая модель с большим количеством параметров стремится к минимизации функции потерь в точках данных и нет никаких гарантий, что она будет себя вести нужным образом \emph{вдали} или \emph{между} точками данных \cite[\strbook{180}]{pytorch-2022}.

Потери на обучающем наборе данных показывают, можно ли вообще подгнать нашу модель к этому обучающему набору данных - другими словами, достаточны ли \emph{разрешающие возможности} (capacity) этой модели для обработки содержащейся в данных информации \cite[\strbook{182}]{pytorch-2022}.

Глубокая нейронная сеть потенциально может аппроксимировать очень сложные функции при условии достаточно большого числа нейронов, а значит, и параметров. Чем меньше параметров, тем проще должна быть форма функции, чтобы наша сеть смогла ее аппроксимировать. Итак, правило 1: если потери на обучающем наборе данных не уменьшаются, вероятно, модель слишком проста \emph{для имеющихся данных}.

Что ж, если вычисленная на проверочном наборе данных функция потерь не убывает вместе с обучающим набором, значит, наша модель обучается лучше аппроксимировать полученные во время обучения примеры данных, но не \emph{обобщается} на примеры данных, которые не входят в этот конкретный набор. Правило 2: если потери на обучающем и проверочном наборах данных расходятся -- модель переобучена.

С интуитвной точки зрения более простая модель может описывать обучающие данные не так хорошо, как более сложная, но зато, вероятно, будет вести себя более равномерым образом между точами данных.

Следовательно, процесс выбора правильного размера нейросетевой модели в смысле количества параметров основан на двух шагах:
\begin{itemize}
	\item увеличение размера до тех пор, пока модель не будет хорошо подогнана к данным,
	
	\item а затем уменьшение, пока не будет устранено переобучение.
\end{itemize}

Разбиение набора данных. Перетасовка элементов тензора эквивалентна перестановке его индексов -- как раз то, что делает функция \verb|randperm|
\begin{lstlisting}[
style = bash,
numbers = none
]
n_samples = t_u.shape[0]
n_val = int(0.2 * n_samples)

shuffled_indicies = torch.randperm(n_samples)
train_indicies = shuffled_indicies[:-n_val]
test_indicies = shuffled_indicies[-n_val:]

train_t_u = t_u[train_indices]
train_t_c = t_c[train_indices]

val_t_u = t_u[val_indicies]
val_t_c = t_c[val_indicies]
\end{lstlisting}

Осталось только дополнительно вычислять потери на проверочном наборе данных \emph{на каждой эпохе}, чтобы заметить переобучение
\begin{lstlisting}[
style = bash,
numbers = none
]
def training_loop(n_epochs, optimizer, params, train_t_u, val_t_u, train_t_c, val_t_c):
    for epoch in range(1, n_epochs + 1):
        train_t_p = model(train_t_u, *params)
        train_loss = loss_fn(train_t_p, train_t_c)
        
        val_t_p = model(val_t_u, *params)
        val_loss = loss_fn(val_t_p, val_t_c)
        
        optimizer.zero_grad()
        # здесь только train_loss.backward(), поскольку мы не хотим обучать 
        # модель на проверочном наборе данных
        train_loss.backward()
        optimizer.step()
        
        if epoch <= 3 or epoch % 500 == 0:
            print(...)
            
    return params
\end{lstlisting}

Запуск
\begin{lstlisting}[
style = bash,
numbers = none	
]
params = torch.tensor([1.0, 0.0], requires_grad=True)
learning_rate = 1e-2
optimizer = optim.SGD([params], lr=learning_rate)

training_loop(
    n_epochs = 3000,
    optimizer = optimizer,
    params = params,
    train_t_u = train_t_un,
    val_t_u = val_t_un,
    train_t_c = train_t_c,
    val_t_c = val_t_c,
)
\end{lstlisting}

Наша цель -- убедиться, что убывают как потери на обучающем наборе данных, \emph{так} и потери на проверочном \cite[\strbook{186}]{pytorch-2022}.

Вопрос: раз мы никогда не вызываем \verb|backward()| для \verb|val_loss|, зачем вообще формировать граф вычислений? Можно просто вызывать \verb|model| и \verb|loss_fn| как обычные функции, без отслеживания истории вычислений.
\begin{lstlisting}[
style = ironpython,
numbers = none
]
def training_loop(n_epochs, optimizer, params, train_t_u, val_t_u, train_t_c, val_t_c):
    for epoch in range(1, n_epochs + 1):
        train_t_p = model(train_t_u, *params)
        train_loss = loss_fn(train_t_p, train_t_c)
        
        with torch.no_grad():
            val_t_p = model(val_t_u, *params)
            val_loss = loss_fn(val_t_p, val_t_c)
            # Убеждаемся, что для нашего вывода аргумент requires_grad == False
            assert val_loss.requires_grad == False
        
        optimizer.zero_grad()
        train_loss.backward()
        optimizer.step()
\end{lstlisting}

Нейрон -- по сути представляет собой линейное преобразование входного сигнала (например, умножение входного сигнала на какое-либо число (\emph{вес}) и прибавление к нему константы \emph{смещения}) с последующим применением фиксированной нелинейной функции (\emph{функции активации}).

Функция активации играет две выжные роли \cite[\strbook{195}]{pytorch-2022}:
\begin{itemize}
	\item Во внутренних частях модели благодаря функции активации возможны различные наклоны графика выходного сигнала в разных значениях -- нечто, по определению \emph{недоступное для линейной функции}. Искусно сочетая эти по-разному наклоненные участки для различных выходных сигналов, нейронные сети могут аппроксимировать любые функции.
	
	\item На последнем слое сети она локализует выходные сигналы предыдущей линейной операции в заданном интервале.
\end{itemize}

Нейрон -- это просто линейная функция с последующей функцией активации. 

Функции активации по определению \cite[\strbook{199}]{pytorch-2022}:
\begin{itemize}
	\item \emph{нелинейны} -- сколько ни применяй преобразование вида $w \cdot x + b$ без функции активации, все равно полчится функция той же самой (аффинной линейной) формы. {\color{blue}\emph{Нелинейность} позволяет сети в целом аппроксимировать более сложные функции},
	
	\item \emph{дифференцируемы}, что дает возможность вычисления градиентов. Точечные разрывы (Hardtanh, ReLU etc.), допустимы.
\end{itemize}

В отсутствие этих характеристик сеть либо превратиться обратно в линейную модель, либо с трудом будет поддаваться обучению.

Для функций активации справедливо следующее:
\begin{itemize}
	\item Имеется по крайней мере один диапазон \emph{чувствительности}, внутри которого нетривиальные изменения входного сигнала приводят к соответствующим нетривиальным изменениям выходного. Необходимо для обучения.
	
	\item У многих из них есть также диапазон \emph{нечувствительности} (\emph{насыщения}), в котором изменения входного сигнала практически не приводят к изменениям выходного.
\end{itemize}

В целом получается механизм, обладающий большими возможностями: при получении на входе различных данных в сети, составленной из \emph{линейных} и \emph{активационных блоков}:
\begin{itemize}
	\item различные нейроны могут возвращать для одних входных сигналов результаты, относящиеся к различным диапазонам,
	
	\item соответствующие этим входным сигналам ошибки в основном влияют на нейроны, работающие в диапазоне чувствительности, а на остальные блоки процесс обучения практически не влияет.
\end{itemize}

В результате объединения множества линейных и активационных блоков параллельно и последовательно получается математический объект, способный аппроксимировать сложные функции. Различные сочетания нейронов реагируют в различных диапазонах на входные сигналы, причем параметры этих блоков можно довольно легко оптимизировать посредством градиентного спуска, поскольку процесс обучения напоминает обучение линейной функции, вплоть до момента насыщения выходного сигнала.

В PyTorch есть отдельный подмодуль, посвященный нейронным сетям, -- \verb|torch.nn|. Он включает <<кирпичики>>, необходимые для создания всех видов нейросетевых архитектур. В терминологии PyTorch эти <<кирпичики>> называются \emph{модулями} (в других фреймворках подобные стандартные блоки часто называются \emph{слоями} (layers)). Модуль PyTorch -- это класс Python, наследующий базовый класс \verb|nn.Module|. 

Подмодули должны быть атрибутами верхнего уровня, а не быть закопаны внутри экземпляров list или dict! В противном случае оптимизатор не сможет их найти (а значит, и их параметры). На случай, если модели потребуется список или ассоциативный массив подмодулей, в PyTorch есть классы \verb|nn.ModuleList| и \verb|nn.ModuleDict|. У \verb|nn.Module| есть подкласс \verb|nn.Linear|, применяющий ко входным сигналам аффинное преобразование.

У всех подклассов \verb|nn.Module| в PyTorch есть метод \verb|__call__|, позволяющий создавать экземпляры \verb|nn.Linear| и вызывать их как функции следующим образом
\begin{lstlisting}[
style = bash,
numbers = none
]
import torch.nn as nn

linear_model = nn.Linear(1, 1)
linear_model(t_un_val)
\end{lstlisting}

Вызов экземпляра \verb|nn.Module| с набором инструментов приводит к вызову метода \verb|forward| с теми же аргументами, который реализует \emph{прямой проход} вычислений, в то время как \verb|__call__| выполняет другие немаловажные операции до и после вызова \verb|forward|. {\color{red} Так что формально можно вызвать \verb|forward| напрямую, и он вернет тот же результат, что и \verb|__call__|, но делать это из пользовательского кода не рекомендуется}
\begin{lstlisting}[
style = bash,
numbers = none
]
y = model(x)  # Правильно!
y = model.forward(x)  # НЕправильно! Так делать не надо!!!
\end{lstlisting}

Конструктор \verb|nn.Linear| принимает три аргумента: число входных принаков, число выходных признаков и булево значение, указывающее, включает линейная модель смещение или нет
\begin{lstlisting}[
style = bash,
numbers = none
]
import torch.nn as nn

linear_model = mm.Linear(1, 1)
linear_model(t_un_val)
\end{lstlisting}

Все модули в \verb|nn| ориентированы на генерацию выходных сигналов сразу для батча из нескольких входных сигналов. Следовательно, если нам нужно выполнить \verb|nn.Linear| для десяти примеров данных, можно создать входной тензор размеров $B \times N_{\text{вх}}$, где $B$ -- размер батча, а $N_{\text{вх}}$ -- число входных признаков, и пропустить его один раз через модель.

Причины для организации данных по батчам многогранны. Одна из них -- желание полноценно загрузить вычислениями имеющиеся вычислительные ресурсы. В частности, GPU позволяют сильно распараллеливать вычисления, так что при одиночном входном сигнале для маленькой модели большинство вычислительных элементов будет простаивать.

Еще одно преимущество в том, что некоторые развитые модели способны использовать статистическую информацию по целому батчу, и эти статистические показатели будут точнее при большом размере батча.

\begin{lstlisting}[
style = bash,
numbers = none
]
linear_model = nn.Linear(1, 1)
optimizer = optim.SGD(
    linear_model.parameters(),
    lr=1e-02,
)
\end{lstlisting}

При вызове метода \verb|training_loss.backward()| в листьях графа вычислений накапливаются градиенты. При вызове \verb|opimizer.step()| программа проходит по всем объектам \verb|Parameter| и меняет их на соответствующую содержимому атрибута \verb|grad| долю. 

\begin{lstlisting}[
style = bash,
numbers = none
]
def training_loop(
    n_epochs,
    optimizer,
    model,
    loss_fn,
    t_u_train,
    t_u_val,
    t_c_train,
    t_c_val,
):
    for epoch in range(1, n_epochs + 1):
        t_p_train = model(t_u_train)
        loss_train = loss_fn(t_p_train, t_c_train)
        
        t_p_val = model(t_u_val)
        loss_val = loss_fn(t_p_val, t_c_val)
        
        # требуется обязательно обнулять градиент на каждой итерации
        optimizer.zero_grad() 
        # выполняем проход в обратном направлении и вычисляем градиенты
        loss_train.backward()
        # обновляем параметры модели
        optimizer.step()
\end{lstlisting}

Модуль \verb|nn| включает несколько распространенных функций потерь, одна из которых -- \verb|nn.MSELoss|.

Модуль \verb|nn| предоставляет удобный способ соединения модулей цепочкой с помощью контейнера \verb|nn.Sequential|
\begin{lstlisting}[
style = bash,
numbers = none
]
seq_model = nn.Sequential(
    nn.Linear(1, 13),
    nn.Tanh(),
    nn.Linear(13, 1)
)
\end{lstlisting}

Модель переходит от одного входного признака до 13 скрытых признаков, пропускает их через функцию активации \verb|Tanh| и, наконец, объединяет получившиеся 13 чисел в один выходной признак.

Названия модулей в \verb|Sequential| представляют собой просто порядковые номера модулей в списке аргументов. Что любопытно, \verb|Sequential| также принимает на входе \verb|OrderDict|, в котором можно указать название каждого из передаваемых \verb|Sequential| модулей
\begin{lstlisting}[
style = bash,
numbers = none
]
from collections import OrderDict

seq_model = nn.Sequential(OrderDict([
    ("hidden_linear", nn.Linear(1, 8)),
    ("hidden_activation", nn.Tanh()),
    ("output_linear", nn.Linear(8, 1))
]))

for name, param in seq_model.named_parameters():
    print(name, param.shape)
# output
hidden_linear.weight torch.Size([8, 1])
hidden_linear.bias torch.Size([8])
output_linear.weight torch.Size([1, 8])
output_linear.bias torch.Size([1])
\end{lstlisting}

Обращаться к конкретным объектам \verb|Parameter| можно путем указания подмодулей в качестве атрибутов
\begin{lstlisting}[
style = bash,
numbers = none	
]
seq_model.output_linear.bias
# output
Parameter containing:
tensor([0.1402], requires_grad=True)
\end{lstlisting}

Можно посмотреть \emph{градиенты параметра} \verb|weight| линейной части скрытого слоя. Запускаем цикл обучения для новой модели нейронной сети, после чего смотрим на получившиеся градиенты после последней эпохи
\begin{lstlisting}[
style = bash,
numbers = none
]
seq_model.hidden_linear.weight.grad
\end{lstlisting}

Функции активации, в дополнение к линейным преобразованиям, позволяют нейронным сетям аппроксимировать сильно нелинейные функции, оставляя их при этом достаточно простыми для оптимизации \cite[\strbook{215}]{pytorch-2022}.

Чтобы преобразовать изображение PIL в тензор PyTorch, можно воспользоваться модулем \verb|torchvision.transforms|. Есть преобразование \verb|ToTensor|, превращающее массивы NumPy и изображения PIL в тензоры. Оно также располагает измерения выходного тензора в порядке $C \times H \times W$ (каналы, высота, ширина).
\begin{lstlisting}[
style = bash,
numbers = none
]
from torchvision import transforms, datasets

cifar10_train = datasets.CIFAR10(data_path, train=True, download=True)
img, label = ciraf10_train[99]

to_tensor = transforms.ToTensor()
img_t = to_tensor(img)
img_t.shape  # torch.Size([3, 32, 32])
\end{lstlisting}

Изображение \verb|img| превратилось в тензор формы $3 \times 32 \times 32$, то есть в изображение размером $32 \times 32$ с тремя цветовыми каналами (RGB).

Это преобразование можно передать непосредственно в виде аргумента \verb|datasets.CIFAR10|
\begin{lstlisting}[
style = bash,
numbers = none
]
tensor_cifar10 = datasets.CIFAR10(
    data_path,
    train=True,
    download=False,
    transform=transforms.ToTensor(),
)
\end{lstlisting}

Рекомендуемая практика -- нормализовать набор данных до нулевого среднего значения и единичного стандартного отклонения по каждому из каналов. 

При выборе функций активации, линейных около нуля ($\pm 1$ или $\pm 2$), ограничение данных тем же диапазоном повышает вероятность \emph{ненулевых градиентов нейронов}, а значит, и \emph{ускоряет обучение}. Кроме того, нормализация каналов к одинаковому распределению гарантирует смешение и обновление информации из разных каналов (посредством градиентного спуска) с одинаковой скоростью обучения.

Чтобы обеспечить нулевое среднее и единичное стандартное отклонение по каждому из каналов, необходимо вычислить среднее значение и стандартное отклонение каждого из каналов набора данных и применить следующее преобразование \verb|v_n[c] = (v[c] - mean[c]) / stdev[c]|. 

Поскольку набор данных CIFAR-10 невелик, можно работать с ним полностью в оперативной памяти. Разместим все возвращаемые объектом \verb|Dataset| тензоры последовательно в дополнительном измерении
\begin{lstlisting}[
style = bash,
numbers = none
]
imgs = torch.stack([img_t for img_t, _ in tensor_cifar_10], dim=3)
imgs.shape  # torch.Size([3, 32, 32, 50 000])
\end{lstlisting}

Теперь можно легко вычислить поканальные средние значения
\begin{lstlisting}[
style = bash,
numbers = none
]
imgs.view(3, -1)  # torch.Size([3, 51200000])
imgs.view(3, -1).mean(dim=1)
imgs.view(3, -1).std(dim=1)
\end{lstlisting}

Теперь
\begin{lstlisting}[
style = bash,
numbers = none
]
transformed_cifar10 = datasets.CIFAR10(
    data_path,
    train=True,
    download=False,
    transform=transforms.Compose([
        transforms.ToTensor(),
        transforsm.Normalize(
            mean=imgs.view(3, -1).mean(dim=1),
            std=imgs.view(3, -1).std(dim=1),
        )
    ])
)
\end{lstlisting}

Сколько признаков содержит каждый пример данных? Так, $3 \times 32 \times 32$ равняется 3072 входных признака на каждый пример. Получаем новую модель \verb|nn.Linear| с 3072 входными признаками и некоторым количеством скрытых принаков, за которым следует функция активации, а затем еще один \verb|nn.Linear|, сокращающий модель до соответствующего количества выходных признаков (в данном случае 2)
\begin{lstlisting}[
style = ironpython,
numbers = none
]
import torch.nn as nn

n_out = 2
model = nn.Sequential(
    nn.Linear(
        3072,  # входные признаки
        512,  # размер скрытого слоя
    ),
    nn.Tanh(),
    nn.Linear(
        512,  # размер скрытого слоя
        n_out,  # выходные признаки
    )
)
\end{lstlisting}

Нейронной сети требуется по крайней мере один скрытый слой (активации, поэтому два модуля) с \emph{нелинейностью} между слоями, чтобы сеть \emph{могла усваивать произвольные функции}, в противном случае модуль будет просто \emph{линейной}.

Здесь необходимо понять, что выходной сигнал носит категориальный характер: птица или самолет. Для представления категориальной величины следует воспользоваться унитарным кодированием, например, \verb|[1, 0]| для самолета и \verb|[0, 1]| для птицы (порядок выбран произвольно). Такая схема подходи и в случае 10 классов, как в полном наборе данных CIFAR-10; просто вектор будет длиной 10.

В нашем частном случае бинарной классификации (птица или самолет; по сути птица или не птица) два значения -- избыточно, поскольку одно всегда равно 1 минус второе. И действительно, PyTorch позволяет выдавать на выходе одно значение \emph{вероятности}, получая вероятность путем использования в конце модели функции активации \verb|nn.Sigmoid| и \underline{функции потерь} на основе \emph{бинарной перекрестной энтропии} \verb|nn.BCELoss|. Существует также \verb|nn.BCELossWithLogits|, объединяющая эти два шага \cite[\strbook{229}]{pytorch-2022}.

В идеальном случае сеть должна выдавать на выходе \verb|torch.tensor([1.0, 0.0])| для самолета и \verb|torch.tensor([0.0, 1.0])| -- для птицы. На практике же, поскольку наш класификатор не будет идеален, следует ожидать от сети неких промежуточных значений. Главное в этом случае, что мы можем интерпретировать выходные сигналы как вероятности: первая запись -- вероятность класса \verb|"airplane"|, а вторая -- \verb|"bird"|.

Чтобы развернуть изображение формы $3 \times 32 \times 32$, а затем преобразовать его в вектор-строку делаем так
\begin{lstlisting}[
style = bash,
numbers = none
]
img_batch = img.view(-1).unsqueeze(0)
img_batch.shape  # torch.Size([1, 3072]); 3 * 32 * 32 = 3072
\end{lstlisting}

Что касается \emph{функции потерь для задач классификации}, то здесь необходимо \emph{накладывать штраф на ошибки классификации}, а не кропотливо штрафовать все, что не равно в точности 0.0 или 1.0, поэтому в задачах классификации плохо работает среднеквадратическая функция потерь.

В этом случае \emph{необходимо максимизировать вероятность}, соответсвующую истинному классу.

{\color{blue}
Другими словами, нам нужна функция потерь, принимающая очень высокие значения, когда \emph{правдоподобие} (истинность параметров нашей модели при имеющихся данных) \emph{низко}: настолько низко, что вероятности альтернативных вариантов выше. И наоборот, потери должны быть низкими, когда правдоподобие данного варианта выше, чем у альтернатив, и мы не хотим зацикливаться на доведении вероятности до 1 \cite[\strbook{234}]{pytorch-2022}.
}

Действующая подобным образом функция потерь существует и называется \emph{отрицательной логарифмической функцией правдоподобия} (negative log likelihood, NLL). 

Для каждого примера данных в батче мы делаем следующее:
\begin{enumerate}
	\item Производим прямой проход и получаем выходные значения из последнего (линейного) слоя.
	
	\item Вычисляем для них многомерную логистическую функцию и получаем вероятности.
	
	\item Извлекаем предсказанную вероятность для истинного класса (правдоподобие параметров). Отметим, что истинный класс известен, поскольку обучение производится с учителем, -- это наши эталнные данные.
	
	\item Вычисляем ее логарифм, ставим перед ним знак <<минус>> и прибавляем к потерям.
\end{enumerate}

Функция \verb|nn.LogSoftmax()| обеспечивает численную устойчивость вычислений
\begin{lstlisting}[
style = bash,
numbers = none
]
model = nn.Sequential(
    nn.Linear(3072, 512),
    nn.Tanh(),
    nn.Linear(512, 2),
    nn.LogSoftmax(dim=1),
)
\end{lstlisting}

Среднеквадратическая функция потерь (MSE) насыщается намного раньше и -- что принципиально -- для совершенно неправильных предсказаний. Основная причина состоит в том, что уклон MSE слишком мал, чтобы компенсировать пологость многомерной логистической функции активации для неправильных предстказаний. {\color{red}Поэтому MSE для вероятностей плохо подходит для задач классификации} \cite[\strbook{237}]{pytorch-2022}.

\begin{lstlisting}[
style = bash,
numbers = none
]
import torch
import torch.nn as nn

model = nn.Sequential(
    nn.Linear(3072, 512),
    nn.Tanh(),
    nn.Linear(512, 2),
    nn.LogSoftmax(dim=1),
)

learning_rate = 1e-2

optimizer = optim.SGD(model.parameters(), lr=learning_rate)

n_epochs = 100

for epoch in n_epochs:
    for img, label in cifar2:
        out = model(img.view(-1).unsqueeze(0))
        loss = loss_fn(out, torch.tensor([label]))
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
   
   print(...)
\end{lstlisting}

Мы поняли, что обработка всех 10 000 изображений одним батчем -- это перебор, так что решили создать внутренний цикл, чтобы \emph{обрабатывать по одному примеру данных за раз} и производить \emph{обратное распространение ошибки по этому одному примеру}.

\emph{Перетасовывая} примеры данных \emph{на каждой эпохе} и вычисляя градиент по одному или (что желательно из соображений устойчивости) нескольким примерам данных за раз, мы фактически вносим элемент случайности в алгоритм градиентного спуска. Оказывается, что следование \emph{градиентам, вычисленным по мини-батчам}, которые представляют собой лишь слабые аппроксимации градиентов, вычисленных по всему набору данных, \emph{улучшает сходимость} и \emph{предотвращает <<застревание>>} процесса оптимизации во встреченных по пути локальных минимумах \cite[\strbook{238}]{pytorch-2022}.

Обычно размер мини-батча представляет собой константу, задаваему до обучения, аналогично скорости обучения. 

Модуль \verb|torch.utils.data| включает класс, помогающий с перетасовкой и организацией данных по мини-батчам: \verb|Dataloader|. Задача загрузчика данных состоит в выборе мини-батчей из набора данных с гибкими возможностями использования различных стратегий выборки. Одна из самых распространенных стратегий: равномерная выборка после \emph{перетасовки} данных \emph{в каждой эпохе}.

\begin{lstlisting}[
style = bash,
numbers = none
]
train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)
\end{lstlisting}

Пример
\begin{lstlisting}[
style = bash,
numbers = none
]
import torch
import torch.nn as nn

train_loader = torch.utils.data.DataLoader(
    cifar2, batch_size=64,
    shuffle=True,
)

model = nn.Sequential(
    nn.Linear(3072, 512),
    nn.Tanh(),
    nn.Linear(512, 2)
    nn.LogSoftmax(dim=1)
)

learning_rate = 1e-2

optimiizer = optim.SGD(model.parameters(), lr=learning_rate)

# отрицательный логарифм правдоподобия
loss_fn = nn.NLLLoss()

n_epochs = 100

for epoch in range(n_epochs):
    for imgs, labels in train_loader:
        batch_size = imgs.shape[0]
        outputs = model(imgs.view(batch_size, -1))
        loss = loss_fn(outputs, labels)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
\end{lstlisting}

На каждой итерации внутреннго цикла \verb|imgs| представляет собой мини-батч 64 RGB-изображений (размером $32 \times 32$), а \verb|labels| -- тензор размером 64 с индексами меток.

Наша цель в том, чтобы правильно присвоить изображениям метки классов, причем желательно на независимом наборе данных
\begin{lstlisting}[
style = bash,
numbers = none
]
val_loader = torch.utils.data.DataLoader(
    cifar2_val,
    batch_size=64,
    shuffle=False,  # NB
)

correct = 0
total = 0

with torch.no_grad():  # NB!
    for imgs, labels in val_loader:
        batch_size = imgs.shape[0]
        outputs = model(imgs.view(batch_size, -1))
        _, predicted = torch.max(outputs, dim=1)
        total += labels.shape[0]
        correct += int((predicted == labels).sum())

print(correct / total)
\end{lstlisting}

Достаточно часто последний слой \verb|nn.LogSoftmax| не включают в сеть, используя в качестве \emph{функции потерь} \verb|nn.CrossEntoryLoss|.

Полносвязная сеть не является \emph{инвариантной относительно сдвига}. Это значит, что сеть, обученная распознавать <<Спитфайер>> (самолет), начинающийся с позиции 4,4, не сможет распознать \underline{\itshape тот же самый} <<Спитфайен>>, начинающийся с позиции 8,8. Нам пришлось бы дополнять (augment) набор данных, то есть применять случайные сдвиги к изображениям во время обучения, чтобы сеть могла заметить <<Спитфайер>> в любом месте изображения, причем это пришлось бы делать для всех изображений в наборе \cite[\strbook{246}]{pytorch-2022}.

Учтите, что \emph{весовые коэффициенты ядра заранее не известны}, а инициализируются \emph{случайным} образом и обновляются посредством обратного распространения ошибки. Отметим также, что для всего изображения используется одно и то же ядро, а это значит, что и весовые коэффициенты ядра \cite[\strbook{252}]{pytorch-2022}. В производную функции потерь по свертрочным весам вности свой вклад все изображение.

Свертка эквивалентна нескольким линейным операциям, весовые коэффициенты которых равны нулю практически везде, кроме окрестностей отдельных пикселей, и получают одинаковые обновления во время обучения.

Очень часто применяют ядра, размеры которых одинаковы по всем измерениям, поэтому в PyTorch есть сокращенная форма записи для них: \verb|kernel_size=3| для двумерной свертки означает форму $3 \times 3$, для трехмерной свертки -- форму $3 \times 3 \times 3$.
\begin{lstlisting}[
style = bash,
numbers = none
]
conv = nn.Conv2d(3, 16, kernel_size=3)
# или так
conv = nn.Conv2d(3, 16, kernel_size=(3, 3))
\end{lstlisting}

Здесь 3 признака на один пиксель (3 канала; RGB) и 16 выходных каналов.

В результате прохода двумерной свертки получается двумерное изображение, пиксели которого представляют собой взвешенную сумму значений по локальным окрестностям входного изображения.

Как начальные значения весовых коэффициентов ядра \verb|conv.weight|, так и смещения задаются случайным образом, так что выходное изображение особого смысла не имеет.

Лучше предерживаться ядер нечентных размеров; ядра четного размера встречаются редко \cite[\strbook{256}]{pytorch-2022}.

Задача сверточной сети состоит в оценке ядра набора фильтров в последовательных слоях, преобразующих многоканальное изображение в другое многоканальное изображение, в котором различные каналы соответствуют разным признакам (например, один канал -- для среднего значения, другой -- для вертикальных краев и т.д.).

Первый набор ядер работает с маленькими окрестностями низкоуровневых признаков первого порядка, а второй набор фактически работает с более широкими окрестностями, \emph{генерируя признаки, представляющие собой композицию предыдущих признаков}. Благодаря этому замечательному механизму сверточные нейронные сети способны анализировать очень сложные кадры.

\begin{lstlisting}[
style = bash,
numbers = none
]
# свертка + нелинейность + пулинг
model = nn.Sequential(
    nn.Conv2d(3, 16, kernel_size=3, padding=1),  # свертка
    nn.Tanh(),  # нелинейность
    nn.MaxPool2d(2),  # пулинг
    nn.Conv2d(16, 8, kernel_size=3, padding=1),
    nn.Tanh(),
    nn.MaxPool2d(2),
    ...
)
\end{lstlisting}

Первая операция свертки превращает три канала RGB в 16, благодаря чему у сети появляется возможность генерировать \emph{16 независимых признаков} (16 каналов). Далее мы применяем функцию активации \verb|Tanh|. Полученное 16-канальное изображение $32 \times 32$ субдискритизируется первым слоем \verb|nn.MaxPool2d| до 16-канального изображения $16 \times 16$. 

Теперь субдискретизированное изображение подвергается еще одной операции свертки, выдающей на выходе 8-канальный выходной сигнал $16 \times 16$. Если повезет, это выходное изображение будет состоять из высокоуровневых признаков. И опять же мы применяем функцию активации \verb|Tanh|, после чего производим субдискретизацию до 8-канального выходного изображения $8 \times 8 $.

После уменьшения входного изображения до набора $8 \times 8$ признаков можно надеяться вернуть из сети значения вероятностей, подходящих для подачи на вход отрицательной логарифмической функции правдоподобия. Нужно преобразовать 8-канальное изображение $8 \times 8$ в одномерный вектор и завершить нашу сеть набором полносвязаных слоев
\begin{lstlisting}[
style = bash,
numbers = none
]
model = nn.Sequential(
    nn,Conv2d(3, 16, kernel_size=3, padding=1),
    nn.Tanh(),
    nn.MaxPool2d(2),
    nn.Conv2d(16, 8, kernel_size=3, padding=1),
    nn.Tanh(),
    nn.MaxPool2d(2),
    ...  # Пропущен важный момент
    nn.Linear(8 * 8 * 8, 32),
    nn.Tanh(),
    nn.Linear(32, 2),
)
\end{lstlisting}

Для \underline{повышения} \emph{разрешающих возможностей модели} можно \emph{увеличить количество выходных каналов} сверточных слоев (то есть число \emph{признаков}, генерируемых каждым из сверточных слоев), в результате чего увеличится и размер линейного слоя.

PyTorch позволяет производить в модели любые вычисления путем создания подклассов \verb|nn.Module|.

Готовые или пользовательские свертки как подмодули обычно включаются в программу посредством описания в конструкторе \verb|__init__| и присваивания их \verb|self| для использования в функции \verb|forward|. Их параметры в то же время храняться в них на протяжении всего жизненного цикла нашего модуля. Обратите внимание, что перед этим необходимо вызывать \verb|super().__init__()|.

\section{Обобщения с помощью сверток}

\subsection{Сеть как подкласс \texttt{nn.Module}}

Задача классификационных сетей обычно заключается в сжатии информации в том смысле, что мы начинаем с изображения, содержащего значительное количество пикселей, и сжимаем его в вектор вероятностей классов.

Для создания подкласса \verb|nn.Module| как минимум необходимо описать функцию \verb|forward|, принимающую входные сигналы модуля и возвращающую выходной.

Подмодули должны быть атрибутами верхнего уровня, а не быть <<закопаны>> внутри экземпляров list или dict! В противном случае оптимизатор не сможет их (а значит, и их параметры) найти. На случай, если модели потребуется список или ассоциативный массив подмодулей, в PyTorch есть классы \verb|nn.ModuleList| и \verb|nn.ModuleDict| \cite[\strbook{269}]{pytorch-2022}. 

\subsection{Функциональные API}

В PyTorch есть \emph{функциональные} аналоги для всех модулей \verb|nn|. Под функциональными здесь подразумевается <<без внутреннего состояния>> -- другими словами, <<выходное значение которых целиком и полностью определяется значениями входных аргументов>>. 

\begin{lstlisting}[
style = ironpython,
numbers = none
]
import torch.nn.functional as F

class Net(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)
        self.fc1 = nn.Linear(8 * 8 * 8, 32)
        self.fc2 = nn.Linear(32, 2)
        
   def forward(self, x):
       out = F.max_pool2d(  # пулинг
           torch.tanh(  # нелинейность
               self.conv1(x)  # свертка
           ), 2)
       out = F.max_pool2d(
           torch.tanh(
               self.conv2(x)
           ), 2)
       out = out.view(-1, 8 * 8 * 8)
       out = torch.tanh(self.fc1(out))
       out = self.fc2(out)
       
       return out
\end{lstlisting}

\subsection{Обучение модели}

В основе сверточной сети лежат два вложенных цикла: внешний -- по \emph{эпохам}, а внутренний -- на основе объекта \verb|DataLoader|, генерирующего батчи из объекта \verb|Dataset|. На каждой итерации цикла необходимо \cite[\strbook{273}]{pytorch-2022}:
\begin{itemize}
	\item Пропустить входные сигналы через модель (прямой проход).
	
	\item Вычислить функцию потерь (также часть прямого прохода).
	
	\item Обнулить все старые градиенты.
	
	\item Вызвать \verb|loss.backward()| для вычисления градиентов функции потерь относительно каждого из параметров (обратный проход).
	
	\item Оптимизировать в сторону уменьшения потерь.
\end{itemize}

\begin{lstlisting}[
style = bash,
numbers = none
]
import datetime

def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):
    for epoch in range(1, n_epochs + 1):
        loss_train = 0.0
        for imgs, labels in train_loader:
            outputs = model(imgs)
            loss = loss_fn(outputs, labels)
            
            # избавляемся от градиентов с предыдущих итераций
            optimizer.zero_grad()
            # выполняем обратный проход; то есть вычисляем градиенты
            # по всем обучаемым параметрам сети
            loss.backward()
            # обновляем модель
            optimizer.step()
            # суммируем потери за этоху
            loss_train += loss.item()
            
     if epoch == 1 or epoch % 10 == 0:
         # получаем средние потери на батч
         print("{} Epoch {}, Training loss {}".format(
             datetime.datetime.now(), epoch,
             loss_train / len(train_loader))
         )
\end{lstlisting}

Обучение в течение 100 эпох
\begin{lstlisting}[
style = bash,
numbers = none
]
# Объект DataLoader организует примеры данных из нашего набора по батчам.
# Перетасовка обеспечивает случайный порядок примеров данных из набора
train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=False)

model = Net()
optimizer = optim.SGD(model.parameters(), lr=1e-02)
loss_fn = nn.CrossEntropyLoss()

training_loop(
    n_epochs=100,
    optimizer=optimizer,
    model=model,
    loss_fn=loss_fn,
    train_loader=train_loader,
)
\end{lstlisting}

Сохранить модель можно так
\begin{lstlisting}[
style = bash,
numbers = none
]
# сохраняютс только веса
torch.save(model.state_dict(), data_path + "birds_vs_airplanes.pt")
\end{lstlisting}

Файл \verb|birds_vs_airplanes.pt| теперь содержит все параметры объекта \verb|model|: весовые коэффициенты и смещения для двух модулей свертки и двух линейных модулей. Никакой структуры, только весовые коэффициенты. Это значит, что при развертывании модели в реальных условиях нам понадобится описание класса \verb|model|
\begin{lstlisting}[
style = bash,
numbers = none	
]
loaded_model = Net()
loaded_model.load_state_dict(torch.load(data_path + "birds_vs_airplanes.pt))
\end{lstlisting}

В \verb|nn.Module| есть реализована функция \verb|.to|, перемещающая все параметры в GPU (или приводящая тип данных, если передать ей аргумент \verb|dtype|). 

Между \verb|Module.to| и \verb|Tensor.to| существует тонкое различие. \verb|Module.to| производит операции с заменой на месте, то есть изменяет экземпляр модуля. А \verb|Tensor.to| -- нет, возвращая \emph{новый тензор}.

Рекомендуемой практикой является создание экземпляра \verb|Optimizer| \emph{после} перемещения всех параметров на нужное устройство \cite[\strbook{275}]{pytorch-2022}.

Перенос вычислений на GPU при его наличии считается хорошим стилем программирования. Неплохим паттерном программирования будет установка значения переменной \verb|device| в зависимости от \verb|torch.cuda.is_avaible|:
\begin{lstlisting}[
style = bash,
numbers = none
]
device = (
    torch.device("cuda") if torch.cuda.is_avaible() else torch.device("cpu")
)
\end{lstlisting}

\begin{lstlisting}[
style = bash,
numbers = none
]
def training_loop(
    n_epochs, optimizer,
    model, loss_fn, train_loader
):
    for epoch in range(1, n_epochs + 1):
        loss_train = 0.0
        for imgs, labels in train_loader:
            imgs = imgs.to(device=device)  # NB
            labels = labels.to(device=device)  # NB
            outputs = model(imgs)
            loss = loss_fn(outputs, labels)
\end{lstlisting}

Можно создать экземпляр модели и перенести ее на \verb|device|
\begin{lstlisting}[
style = bash,
numbers = none
]
train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)

# перенести модель (все ее параметры) на GPU
model = Net().to(device=device)
optimizer = optim.SGD(model.parameters(), lr=1e-02)
loss_fn = nn.CrossEntropyLoss()

training_loop(
    n_epochs=100,
    optimizer=optimizer,
    model=model,
    loss_fn=loss_fn,
    train_loader=train_loader,
)
\end{lstlisting}

Если забыть перенести саму модель или входные данные на GPU, вы получите сообщения об ошибках, указывающие, что тензоры располагаются на различных устройствах, поскольку операторы PyTorch не поддерживают смеси входных данных на GPU и CPU.

PyTorch попытается загрузить веса на то же устройство, с которого они были сохранены, то есть весовые коэффициенты с GPU будут восстановлены на GPU. Лаконичным вариантом будет потребовать от PyTorch переопределить информацию об устройстве при загрузке весовых коэффициентов
\begin{lstlisting}[
style = bash,
numbers = none
]
loaded_model = Net().to(device=device)
loaded_model.load_state_dict(
    torch.load(
        data_path + "birds_vs_airplanes.pt",
        map_location=device
    )
)
\end{lstlisting}

\subsection{Ширина сети}

Ширина сети это количество нейронов в слое или каналов на каждую операцию свертки. Расширить модель в PyTorch очень легко. Необходимо просто указать большее количество выходных каналов в первой свертке и увеличивать следующие слои соответствующим образом, не забывая менять функцию \verb|forward| так, чтобы отразить увеличивщуюся длину вектора при переходе на полносвязные слои.

Чем больше разрешающих возможностей модели, тем с большей степенью изменчивости входных сигналов сможет справиться модель, но в то же время тем выше вероятность переобучения, поскольку модель сможет воспользоваться дополнительными параметрами для запоминания несущественных аспектов входных данных. 

\subsection{Регуляризация}

Основные способы регуляризации:
\begin{itemize}
	\item штрафы на весовые коэффициенты ($L_1$, $L_2$-норма),
	
	\item дропаут,
	
	\item нормализация по батчам (альтернатива дропауту).
\end{itemize}

Первый способ достижения устойчивости обобщения: добавления члена регуляризации в формулу потерь. Этот дополнительный член ограничивает рост весовых коэффициентов модели в процессе обучения: он устроен так, что они стремятся оставаться маленькими. Другими словами, он налагает штраф на большие значения весов. В результате форма функции потерь становится более гладкой, и для модели нет особого смысла подстраиваться под отдельные примеры данных.

Наиболее популярные виды членов регуляризации: $L_2$-регуляризация и $L_1$-регуляризация. $L_2$-регуляризацию также называют \emph{затуханием весов} (weight decay). Прибавление к функции потерь члена $L_2$-регуляризации эквивалентно уменьшению каждого весового коэффициента пропорционально его текущему значению во время шага оптимизации (отсюда и название <<затухание веса>>). Обратите внимание, что затухание веса относится ко всем параметрам сети, в том числе и к смещениям.

\begin{lstlisting}[
style = ironpython,
numbers = none
]
def training_loop_l2reg(n_epochs, optimizer, model, loss_fn, train_loader):
    for epoch in range(1, n_epochs + 1):
        loss_train = 0.0
        for imgs, labels in train_loader:
            imgs = imgs.to(device)
            labels = labels.to(device=device)
            outputs = model(imgs)
            loss = loss_fn(outputs, labels)
            
            l2_lambda = 0.001
            ls_norm = sum(p.pow(2.0).sum() for p in model.parameters())
            loss = loss + l2_lambda * l2_norm
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            loss_train += loss.item()
\end{lstlisting}

Впрочем, в оптимизаторе SGD в PyTorch уже есть параметр \verb|weight_decay|, соответствующий \verb|2 * lambda|, который напрямую осуществляет затухание весов во время их обновления. Он полностью эквивалентен прибавлению $L_2$-нормы весовых коэффициентов к функции потерь без необходимости накопления в функции потерь и вовлечения автоматического вычисления градиентов.

Идея дропаута действительно проста: обнуляем случайную часть выходных сигналов нейронов по сети, причем этот случайный выбор производится на каждой итерации обучения. 

Фактически в результате этой процедуры на каждой итерации формируются слегка отличающиеся модели с различными топологиями нейронов, уменьшая шансы нейронов модели скоординироваться в процессе запоминания, что происходит при переобучении.

В PyTorch можно реализовать дропаут в модели с помощью добавления модуля \verb|nn.Dropout| между нелинейной функцией активации и линейным или сверточным модулем последующего слоя. В качестве аргумента необходимо указать вероятность, с которой будут обнуляться входные сигналы.
\begin{lstlisting}[
style = bash,
numbers = none
]
class NetDropout(nn.Module):
    def __init__(self, n_chans1=32):
        super().__init__()
        self.n_chans1 = n_chans1
        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)
        self.conv1_dropout = nn.Dropout2d(p=0.4)
        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, padding=1)
        self.conv2_dropout = nn.Dropout2d(p=0.4)
        self.fc1 = nn.Linear(8 * 8 * n_chans1 // 2, 32)
        self.fc2 = nn.Linear(32, 2)
        
    def forward(self, x):
        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)
        out = self.conv1_dropout(out)  # Dropout
        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)
        out = self.conv2_dropout(out)  # Dropout
        out = self.view(-1, 8 * 8 * self.n_chans1 // 2)
        out = torch.tanh(self.fc1(out))
        out = self.fc2(out)
        
        return out
\end{lstlisting}

\emph{Дропаут} обычно происходит \emph{во время обучения}, в то время как \emph{во время использования} обученной модели в реальных условиях \emph{модуль дропаута обходят} или, что эквивалентно, присваивают равную нулю вероятность. Этот процесс контролируется свойством \verb|train| модуля \verb|Dropout| \cite[\strbook{283}]{pytorch-2022}.

PyTorch позволяет переключаться между двумя режимами, вызывая \verb|model.train()| или \verb|model.eval()| для любого подкласса \verb|nn.Model|.

В статье Сергея Йоффе <<Нормализация по батчам: ускорение обучения нейронных сетей путем сокращения внутреннего ковариантного сдвига>> (Batch Normalization: Accelerating Deep Network Trainging by Reducing Internal Covariate Shift) описывается методика, которая позволяет \emph{повысить скорость обучения} и \emph{снизить зависимость обучения от начальных значений}, а также играет роль \underline{\itshape регуляризатора}, тем самым представляя альтернативу дропауту \cite[\strbook{283}]{pytorch-2022}.

Основная идея \emph{нормализации по батчам} состоит в нормализации входных сигналов функций активации сети так, чтобы получить определенное желательное распределение для мини-батчей. Если вспомнить внутренние механизмы обучения и роль нелинейных функций активации, становится ясно, что это помогает \emph{избежать чрезмерного углубления входных сигналов функций активации в область насыщения}, что гасит градиенты и замедляет обучение \cite[\strbook{283}]{pytorch-2022}.

На практике нормализация по батчам сдвигает и масштабирует промежуточные входные сигналы на основе среднего значения и стандартного отклонения, вычисленых в этой промежуточной точке по примерам данных мини-батча. Эффект от регуляризации основан на том, что отдельные примеры данных и следующие далее по конвейеру фунцкии активации всегда рассматриваются моделью как сдвинутые и нормализованные, в зависимости от статистических показателей выделенного случайным образом мини-батча. Авторы статьи полагают, что нормализация по батчам исключает или по крайней мере сокращает необходимость в дропауте.

Поскольку цель нормализации по батчам -- масштабировать входные сигналы функции активации, логично будет производить ее после линейного преобразования (свертки а данном случае), то есть перед функцией активации
\begin{lstlisting}[
style = ironpython,
numbers = none	
]
class NetBatchNorm(nn.Module):
    def __init__(self, n_chans1=32):
        super().__init__()
        self.n_chans1 = n_chans1
        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)
        self.conv1_batchnorm = nn.BatchNorm2d(num_features=n_chans1)
        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, padding=1)
        self.conv2_bachnorm = nn.BatchNorm2d(num_features=n_chans1 // 2)
        self.fc1 = nn.Linear(8 * 8 * n_chans1 // 2, 32)
        self.fc2 = nn.Linear(32, 2)
        
    def forward(self, x):
        out = self.conv1_batchnorm(self.conv1(x))  # batch norm
        out = F.max_pool2d(torch.tanh(out))
        out = self.conv2_batchnorm(self.conv2(out))  # batch norm
        out = out.view(-1, 8 * 8 * self.n_chans1 // 2)
        out = torch.tanh(self.fc1(out))
        out = self.fc2(out)
        
        return out
\end{lstlisting}

Как и дропаут, нормализация по батчам должна вести себя по-разному во время обучения и во время выполнения вывода. На самом деле \emph{во время выполнения вывода} желательно, чтобы выходной сигнал для конкретного входного сигнала не зависел от прочих входных сигналов, подаваемых на вход модели. А это значит, что необходим способ нормализовать данные, но при этом \emph{раз и навсегда зафиксировать параметры нормализации}.

При обработке мини-батчей, помимо оценки среднего значения и стандартного отклонения для текущего мини-батча, PyTorch также обновляет в качестве приближения \emph{скользящие оценки среднего значения и стандартного отклонения}, репрезентативные для всего набора данных.

Таким образом, если пользователь указывает \verb|model.eval()| и модель содержит модуль нормализации по батчам, скользящие оценки фиксируются и используются для нормализации. Для возврата к использованию статистических показателей мини-батчей мы вызываем \verb|model.train()| точно так же, как для дропаута.

\emph{Глубина} обеспечивает возможность работы сети с \emph{иерархической информацией}, когда для анализа какого-либо входного сигнала необходимо понимать \emph{конекст} \cite[\strbook{285}]{pytorch-2022}.

Углубление сети обычно ухудшает сходимость. Производные функции потерь относительно параметров, особенно в первых слоях, приходится умножать на множество других чисел, генерируемых цепочкой операций между функцией потерь и параметром. Эти множители могут быть маленькими, приводя в результате к еще меньшим числам, или большими, поглощая маленькие числа из-за приближенности операций с плавающей запятой. В сухом остатке мы получаем, что в результате длинной цепочки операций умножение \emph{вклад отдельного параметра в градиент \underline{исчезает}}, и это ведет к \emph{неэффективному обучению} данного слоя, поскольку ни этот, ни другие параметры не будут обновляться должным образом.

Добавление в модель обходной связи (skip connection) наподобие ResNet сводится к прибавлению выходного сигнала какого-то слоя к входному сигналу другого слоя
\begin{lstlisting}[
style = bash,
numbers = none
]
class ResNet(nn.Module):
    def __init__(self, n_chans1=32):
        super().__init__()
        self.n_chans1 = n_chans1
        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(n_chans1 // 2, n_chans1 // 2, kernel_size=3, padding=1)
        self.fc1 = nn.Linear(4 * 4 n_chans1 // 2, 32)
        self.fc2 = nn.Linear(32, 2)
        
    def forward(self, x):
        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)
        out = F.max_pool2d(torch.relu(self.conv2(out)), 2)
        out1 = out
        out = F.max_pool2d(torch.relu(self.conv3(out)) + out1, 2)  # skip connection
        out = out.view(-1, 4 * 4 * self.n_chans1 // 2)
        out = self.fc2(out)
        
        return out
\end{lstlisting}

Обходная связь создает прямой путь от расположенных глубоко параметров к функции потерь, благодаря чему они вносят более непосредственный вклад в градиент функции потерь, ведь частные производные функции потерь по этим параметрам теперь получают шанс не умножаться на коэффициент в длинной цепочке прочих операций.

Отмечается, что обходные связи благотворно влияют на сходимость, особенно на начальных этапах обучения. Кроме того, поверхность функции потерь глубоких остаточных сетей намного глаже, чем у сетей прямого распространения той же глубины и ширины.

Применение обходных связей в ResNet дало возможность успешно обучать модели глубиной более 100 слоев.

\verb|nn.Sequential| гарантирует, что выходной сигнал одного блока будет использован как входной сигнал следующего, а также что все параметры блока видимы \verb|Net|.

\begin{lstlisting}[
style = bash,
numbers = none
]
class ResBlock(nn.Module):
    def __init__(self, n_chans):
        super().__init__()
        # Слой BatchNorm свел бы на нет эффект смещения, так что смещение обычно опускают
        self.conv = nn.Conv2d(n_chans, n_chans, kernel_size=3, padding=1, bias=False)
        self.batch_norm = nn.BatchNorm(n_num_features=n_chans)
        torch.nn.init.kaiming_normal_(self.conv.weight, nonlinearity="relu")
        torch.nn.init.constant_(self.batch_norm.weight, 0.5)
        torch.nn.init.zeros_(self.batch_norm.bias)
        
    def forward(self, x):
        out = self.conv(x)
        out = self.batch_norm(out)
        out = torch.relu(out)
        
        return out + x

class NetResDeep(nn.Module):
    def __init__(self, n_chans1=32, n_blocks=10):
        super().__init__()
        self.n_chans1 = n_chans1
        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)
        self.resblocks = nn.Sequential(
            *(n_blocks * [ResBlock(n_chans=n_chans1)]))
        self.fc1 = nn.Linear(8 * 8 * n_chans1, 32)
        self.fc2 = nn.Linear(32, 2)
        
    def forward(self, x):
        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)
        out = self.resblocks(out)
        out = F.max_pool2d(out, 2)
        out = out.view(-1, 8 * 8 * self.n_chans1)
        out = torch.relu(self.fc1(out))
        out = self.fc2(out)
        
        return out
\end{lstlisting}

У регуляризации весов и дропаута статистическая интерпретация в качестве методов регуляризации более строграя, чем нормализация по батчам. Нормализация по батчам предназначена скорее для \emph{улучшения сходимости} \cite[\strbook{292}]{pytorch-2022}.

\section{Применение PyTorch в борьбе с раком}

Компьютерная томография -- это, по сути, трехмерные рентгеновские снимки, представленные в виде трехмерного массива одноканальных данных изображений.

Воксель -- трехмерный эквивалент привычного двумерного пикселя. Он занимает некий объем пространства, а не плоскую область и обычно размещается в трехмерной сетке.

Каждый воксель КТ имеет числовое значение, которое примерно соответствует \emph{средней массовой плотности вещества}, содержащегося в этой точке. На большинстве визуализаций подобных данных вещества высокой плотности, такие как кости и металлические импланты, отображаются белым, воздух и легочная ткань с малой плотностью -- черной, а жир и ткани -- различными оттенками серого.

Основное различие между компьютерной томографией и рентгеновскими снимками заключается в том, что ренгеновский снимок представляет собой проекцию трехмерной интенсивности (в данном случае плотности ткани и костей) на двумерную плоскость, а компьютерная томография сохраняет данные в третьем измерении.

Компьютерная томография фактически измеряет радиоплотность, которая является функцией как массовой плотности, так и атомного номера исследуемого вещества.

\subsection{Файлы необработанных данных КТ}

Данные КТ у нас представлены в двух видах файлов: файлах \verb|.mhd|, содержащих метаданные заголовков, и файлах \verb|.raw|, содержащих необработанные байты, в виде трехмерных массивов. Имя каждого файла начинается с уникального идентификатора, называемого UID (название происходит от номенклатуры цифровых изображений и коммуникаций в медицине -- Digital Imaging and Communecations in Medicine, DICOM) для компьютерной томографии. Например, UID 1.2.3 соответствуют два файла: \verb|1.2.3.mhd| и \verb|1.2.3.raw|.

\subsection{Обучающие и проверочные данные}

Для любой стандартной задачи обучения с учителем данные деляться на обучающие и проверочные наборы. Оба должны быть \emph{репрезентативны} для диапазона реальных входных данных, которые мы ожидаем получить и хотим обрабатывать. Если какой-либо из наборов существенно отличается от реальных вариантов использования, то вполне вероятно, что наша модель будет вести себя не так, как мы ожидаем, поскольку модель, обученная на далеких от реальности данных, не сможет работать нормально в полевых условиях!

\subsection{Единицы Хаунсфилда}

Воксели КТ выражены в \emph{единицах Хаунсфилда} (HU), где воздух имеет значение -1000 HU, вода составляет 0 HU, а кость -- не менее +1000 HU. Некоторые компьютерные томографы используют значения HU, соответствующие отрицательной плотности, указывая с их помощью на воксели, находящиеся за пределами поля зрения компьютерного томографа. Для наших целей все, что находится за пределами пациента, должно считаться воздухом, поэтому мы отбрасываем информацию о поле зрения, устанавливая нижнюю границу значений на уровне - 1000 HU. Аналогично точная плотность костей, металлических имплантов и так далее нас тоже не интересует, так что мы ограничиваем ее значением примерно 2 $\text{г} / \text{см}^3$ (1000 HU), хотя в большинстве случаев это биологически неточно.

Следует удалить из наших данных все выбивающиеся значения. Они не нужны для нашей цели, а их наличие может усложнить работу модели. Это усложнение может произойти по-разному, но чаще всего возникает ситуация, когда при пакетной нормализации выбивающиейся значения искажают данные.

К сожалению, все данные центра кандидата, выражены в милимитрах, а не в вокселях! Нам нужно преобразовать наши координаты из миллимитровой системы координат ($X, Y, Z$), в которой они выражены, в систему координат, основанную на \emph{адресах вокселей} ($I, R, C$), используемую для получения срезов массива из данных компьютерной томографии.

В \emph{системе координат пациента} положительное значение $X$ определяется как направление к левой стороне тела пациента (влево), положительное значение $Y$ -- как направление к спине (назад) и положительное значение $Z$ -- как направление к голове пациента (вверх). Эту систему иногда сокращенно называют LPS -- left-posterior-superior, или <<влево -- назад -- вверх>> \cite[\strbook{333}]{pytorch-2022}.

Система координат пациента измеряется в миллиметрах, а начало координат в ней расположено произвольно и не соответствует началу координат массива воксеелей.

Многие компьютерные томографы часто отличаются друг от друга размером вокселей, которые обычно не кубической формы. Они могут иметь размеры, например, $1.125 \times 1.125 \times 2.5\, \text{мм}$.

КТ-сканы обычно имеют размерность 512 строк на 512 столбцов, а по оси индексов обычно бывает от 100 полных срезов до, возможно, 250 срезов (250 срезов по 2.5 мм обычно достаточно, чтобы охватить интересующую область).

Заставить модель исследовать такие огромные массивы данных в поисках намеков на нужные нам узелки -- это все равно что попросить вас найти одно слово с ошибкой в сборнике романов, написанных на незнакомом вам языке.

Вместо этого выделим область вокруг каждого кандидата и заставим модель рассматривать кандидатов по одному. 

PyTorch API требует, чтобы любые подклассы \verb|Dataset|, которые мы хотим реализовать, должны иметь две функции:
\begin{itemize}
	\item \verb|__len__|, которая после инициализации должна возвращать постоянное значение,
	
	\item \verb|__getitem__|, которая принимает индекс и возвращает кортеж с демонстрационными данными.
\end{itemize}

Чтобы \verb|LunaDataset| работал достаточно эффективно, нужно кэшировать результаты на диске. 
\begin{lstlisting}[
style = bash,
numbers = none
]
@functools.lru_cache(1, typed=True)
def getCt(series_uid):
    return Ct(series_uid)
    
@raw_cache.memoize(typed=True)
def getCtRawCandidate(series_uid, center_xyz, width_irc):
    ct = getCt(series_uid)
    ct_chunk, center_irc = ct.getRawCandidate(center_xyz, width_irc)
    
    return ct_chunk, center_irc
\end{lstlisting}

Здесь мы используем несколько различных методов кэширования. Прежде всего мы кэшируем возвращаемое значение \verb|getCt| в памяти, чтобы можно было многократно запрашивать один и тот же экземпляр \verb|Ct|, не загружая заново все данные с диска. Это даст огромный прирост скорости в случае повторяющихся запросов, но мы \underline{сохраняем в памяти только один КТ}, поэтому промахи кэша будут частыми, если мы не будем следить за порядком доступа \cite[\strbook{342}]{pytorch-2022}.

Однако функция \verb|getCtRawCandidate|, которая вызвает \verb|getCt|, также кэширует свои выходные данные; поэтому после того, как наш кэш будет заполнен, функция \verb|getCt| вызываться не будет. Эти значения \emph{кэшируются на диск} с помощью библиотеки \verb|diskcache| \url{https://grantjenks.com/docs/diskcache/}.

Библиотека DiskCache использует дисковое пространство для кэширования! Объект \verb|Cache| потокобезопасный и потому его можно разделять между несколькими потоками. Два \verb|Cache| объекта могут ссылаться на одну и ту же директорию из разных потоков или процессов. Все операции с кэшем в отличие от файлов являются \emph{атомарными}.

\begin{lstlisting}[
style = ironpython,
numbers = none
]
from diskcache import Cache

cache = Cache()

with Cache(cache.directory) as reference:
    reference.set("key", "value")
\end{lstlisting}

Закрытые \verb|Cache|-объекты откроются снова при доступе к ним. Но открытие \verb|Cache|-объекта занимает относительно много времени, поэтому можно кэш не закрывать.

Работа с кэшем выглядит как с обычным словарем Python
\begin{lstlisting}[
style = bash,
numbers = none
]
cache["key"] = "value"
cache["key"]  # 'value'
"key" in cache  # True
del cache["key"]
\end{lstlisting}

Или так
\begin{lstlisting}[
style = bash,
numbers = none
]
from io import BytesIO
cache.set("key", BytesIO(b"value"), expire=5, read=True, tag="data")
\end{lstlisting}

Здесь ключ перестает быть доступным через 5 секунд, значение представляется как файловый объект и назначается тэг.

Прочитать значение по ключу можно так
\begin{lstlisting}[
style = bash,
numbers = none
]
cache.get("key", read=True, expire_time=True, tag=True)
\end{lstlisting}

Метод \verb|.touch()| используется для обновления значения параметра \verb|expire|
\begin{lstlisting}[
style = bash,
numbers = none
]
cache.touch("key", expire=None)
\end{lstlisting}

Метод \verb|.add()| может использоваться для вставки пары в кэш. Пара вставляется только если ключ не существовал раньше.

\begin{lstlisting}[
style = bash,
numbers = none
]
cache.add(b"test", 123)  # True
cache[b"test"]
cache.add(b"test", 456)  # False
cache[b"test"]  # 123
\end{lstlisting}

Значения ключей можно изменять через инкремент / декремент (\verb|.incr()|, \verb|.decr()|).

\verb|FanoutCache| автоматически сегментирует базу данных. 
\begin{lstlisting}[
style = bash,
numbers = none
]
from diskcache import FanoutCache

cache = FanoutCache(shards=4, timeout=1)
\end{lstlisting}

Здесь создается кэш во временной директории с 4 сегментами и 1 секундой ожидания. Есть еще декоратор \verb|memoize|
\begin{lstlisting}[
style = ironpython,
numbers = none
]
from diskcache import FanoutCache
cache = FanoutCache()
@cache.memoize(typed=True, expire=1, tag='fib')
def fibonacci(number):
	if number == 0:
		return 0
	elif number == 1:
		return 1
	else:
		return fibonacci(number - 1) + fibonacci(number - 2)
print(sum(fibonacci(value) for value in range(100)))  # 573147844013817084100
\end{lstlisting}

Размер кэша на диске задается с помощью параметра \verb|size_limit|
\begin{lstlisting}[
style = bash,
numbers = none
]
cache = Cache(size_limit=int(4e9))  # 4 Gb
\end{lstlisting}

Считывать с диска $2^{15}$ значения типа \verb|float32| намного быстрее, чем читать $2^{25}$ значений типа \verb|int16|, преобразовывать их в \verb|float32|, а затем выбирать из них $2^{15}$ значений. Начиная со второго прохода данных, время ввода-вывода для ввода должно сократиться до незначительного значения.

Если в системе установлено более одного графического процессора, то мы задействуем класс \verb|nn.DataParallel| для распределения работы между всеми графическими процессорами в системе, затем собираем и повторно синхронизируем обновления параметров и т.д.
\begin{lstlisting}[
style = bash,
numbers = none
]
...
if torch.cuda.device_count() > 1:
    model = nn.DataParallel(model)
    model = model.to(self.device)
...
\end{lstlisting}

Вызов \verb|model.to(self.device)| перемещает параметры модели в графический процессор, настраивая свертки и другие вычисления с целью использовать графический процессор для тяжелой вычислительной работы.

Важно сделать это \underline{перед} созданием оптимизатора, поскольку в противном случае оптимизатору придется работать с объектами в центральном процессоре, а не с объектами, скопированными в графический процессор \cite[\strbook{355}]{pytorch-2022}.

Здесь рассматривается случай использования нескольких графических процессоров с помощью класса \verb|DataParallel|. Им легко обернуть уже имеющиеся модели. Но в целом этот способ применения нескольких графических процессоров не является самым эффективным и ограничен работой с оборудованием, имеющимся на одной машине.

В PyTorch также есть класс \verb|DistributedDataParallel|, который рекомендуется использовать в случаях, когда вам нужно распределить работу между \emph{несколькими} графическими процессорами или машинами. Правильно выполнить его настройку довольно непросто.

SGD довольно часто используется в качестве первого оптимизатора. В некоторых задачах SGD может работать плохо, но такие задачи относительно редки. Аналогично скорость обучения 0.001 и импульс 0.9 -- достаточно безопасные стартовые значения. Опыт свидетельствует, что SGD с этими значениями хорошо показал себя в довольно широком круге проектов.

Нам не нужно реализовывать пакетную обработку, поскольку класс PyTorch \verb|DataLoader| умеет это делать. Мы уже построили преобразование из КТ-сканов в тензоры PyTorch с помощью класса \verb|LunaDataset|, поэтому осталось лишь подключить наш набор данных к загрузчику
\begin{lstlisting}[
style = bash,
numbers = none
]
def initTrainDl(self):
    train_ds = LunaDataset(  # пользовательский набор данных
        val_stride=10,
        isValSet_bool=False,
    )
    
    batch_size = self.cli_args.batch_size
    if self.use_cuda:
        batch_size *= torch.cuda.device_count()
        
    trail_dl = DataLoader(
        train_ds,
        batch_size=batch_size,  # Разбиение на пакеты выполняется автоматически
        num_workers=self.cli_args.num_workers,
        pin_memory=self.use_cuda,  # область памяти перемещается в GPU
    )
\end{lstlisting}

В дополнение к пакетной обработке отдельных образцов загрузчики данных (\verb|DataLoader|) также могут обеспечивать \emph{параллельную} загрузку данных с помощью \underline{отдельных процессов} и общей памяти. Все, что нам нужно сделать, -- это указать \verb|num_workers=| при создании экземпляра загрузчика данных \cite[\strbook{358}]{pytorch-2022}.

\subsection{Первый сквозной дизайн нейронной сети}

В каждом блоке одинаковый (или по крайней мере схожий) набор слоев, хотя часто размер входных данных и количество фильтров у блоков различаются.

\begin{lstlisting}[
style = ironpython,
numbers = none
]
class LunaBlock(nn.Module):
    def __init__(self, in_channels, conv_channels):
        super().__init__()
        self.conv1 = nn.Conv3d(
            in_channels, conv_channels, kernel_size=3, 
            padding=1, bias=True,
        )
        
        self.relu1 = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv3d(
            conv_channels, conv_channels,
            kernel_size=3, padding=1, bias=True
        )
        self.relu2 = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool3d(2, 2)
        
    def forward(self, input_batch):
        block_out = self.conv1(input_batch)
        block_out = self.relu1(block_out)
        block_out = self.conv2(block_out)
        block_out = self.relu2(block_out)
        
        return self.maxpool(block_out)
\end{lstlisting}

Если выходной воксель подается в другое ядро $3 \times 3 \times 3$ в качестве одного из краевых вокселей, то часть входных данных первого слоя будет находится за пределами области $3 \times 3 \times 3$ второго рода. Конечный результат двух сложенных слоев имеет эффективное рецептивное поле $5 \times 5 \times 5$. Это означает, что взятые вместе слои действуют так же, как один сверточный слой большего размера \cite[\strbook{362}]{pytorch-2022}.

Иными словами, каждый сверточный слой $3 \times 3 \times 3$ добавляет дополнительный одновоксельный слой по краю рецептивного поля. 

Два сложенных слоя $3 \times 3 \times 3$ используют меньше параметров, чем полная свертка $5 \times 5 \times 5$ (и, следовательно, вычисление также выполняется быстрее).

Чтобы добиться хорошей производительности модели, веса, смещения и другие параметры сети должны обладать определенными свойствами. Представим себе вырожденный случай, когда все веса сети больше 1 (и у нас нет остаточный связей). В такой случае повторное умножение на эти веса приведет к тому, что выходные данные слоя будут расти по мере прохождения данных через слои сети. Аналогично, вес меньшей 1 приведет к уменьшению и исчезновению выходных данных всех слоев. Такие же соображения применимы к градиентам в обратном проходе \cite[\strbook{365}]{pytorch-2022}.

Обеспечить правильное поведение выходных данных слоя можно с помощью множества методов нормализации. Один из самых простых -- убедиться, что веса сети инициализированы таким образом, чтобы промежуточные значения и градиенты не становились ни неоправданно малыми, ни неоправданно большими. 
\begin{lstlisting}[
style = bash,
numbers = none
]
def _init_weights(self):
    for m in self.modules():
        if type(m) in {nn.Linear, nn.Conv3d}:
            nn.init.kaiming_normal_(
                m.weight.data, a=0, mode="fan_out", nonlinearity="relu"
            )
            if m.bias is not None:
                fan_in, fan_out = nn.init._calculate_fan_in_and_fan_out(m.weight.data)
                bound = 1 / math.sqrt(fan_out)
                nn.init.normal_(m.bias, - bound, bound)
\end{lstlisting}

Метод \verb|.detach()|, то есть <<отсоединение>> используется тогда, когда объект не должен удерживать градиенты \cite[\strbook{369}]{pytorch-2022}.

Цикл проверки работает аналогично. Кроме того, код работает быстрее благодаря использованию контекстного менеджера \verb|with torch.no_grad()|, явно информирующего PyTorch о том, что \emph{не нужно вычислять градиенты}.

Простой способ определить наличие узкого места в загрузке данных или вычислениях -- подождать несколько секунд, а затем вызвать \verb|top| или \verb|nvidia-smi|:
\begin{itemize}
	\item если 8 рабочих процессов Python загружают более 80\% ЦП, то, вероятно, вам необходимо подготовить кэш,
	
	\item если \verb|nvidia-smi| сообщает, что значение \verb|GPU-Util| более 80\%, ГП загружен работой.
\end{itemize}

Наша цель -- насытить ГП работой, то есть мы хотим использовать как можно больше его вычислительной мощности для быстрого завершения эпох.

Для визуализации метрик удобно использовать \verb|TensorBoard| (требуется установить пакет \verb|tensorflow|). Можно установить стандартный пакет только для ЦП.

Рекомендуется разделить ваши данные по отдельным папкам, так как \verb|TensorBoard| несколько загромождается, если вы проведете более 10 или 20 экспериментов. 
\begin{lstlisting}[
style = bash,
numbers = none
]
$ tensorboard --logdir ./runs
\end{lstlisting}

Точность и отклик (полнота) -- это ценные показатели, которые можно отслеживать во время обучения. Если любой из них упадет до нуля, то стоит предположить, что модель начала вести себя вырожденным образом \cite[\strbook{401}]{pytorch-2022}.

В данном  случае соотношение положетиельных точек к отрицательным составляет 400:1. Это \emph{катасторфически} большой дисбаланс! Неудевительно, что настоящие узелки попросту теряются в общей массе!

В качестве функции потерь будем использовать \verb|nn.CrossEntropyLoss|, которая технически оперирует необработанными логитами, а не вероятностями класса.

Прогнозы, численно близкие к правильной метке, не приводят к значительному изменению весов сети, а прогнозы, значительно отличающиеся от правильного ответа, влияют на веса гораздо сильнее.

Наш обучающий набор нужно будет отредактировать так, чтобы в нем \emph{чередовались} положительные и отрицательные элементы.

Мы \emph{не будем проводить балансировку \underline{проверочного}} (тестового) набора. Наша модель должна хорошо работать в реальном мире, а он несбалансирован (в конце концов, именно из реального мира мы получили необработанные данные!) \cite[\strbook{414}]{pytorch-2022}.

Мы \emph{дополняем} набор данных, применяя синтетические изменения к отдельным элементам, в результате чего получается новый набор данных, превышающий по размеру исходный. Обычно цель дополнения -- получить синтетический набор, который остается репрезентативным для того же общего класса, что и исходный, но который нельзя тривиально запомнить вместе с оригиналом. При правильном выполнении это увеличение может увеличить тренировочный набор настолько, что модель не сможет его запомнить, и тогда она будет вынуждена больше полагаться на обобщение.

Методы дополнения (аугментации) данных:
\begin{itemize}
	\item зеркальное отражение изображения вверх-вниз, влево-вправо и/или вперед-назад,
	
	\item сдвиг изображения на несколько вокселей,
	
    \item масштабирование изображения вверх или вниз,
    
    \item вращение изображения вокруг оси <<голова -- нога>>,
    
    \item добавление шума.
\end{itemize}

Для каджого метода нужно убедиться, что выбранный подход сохраняет репрезентативный характер измененного обучающего элемента, но в то же время отличается достаточно, чтобы этот элемент был полезен для обучения.

NB: Важно структурировать конвейер данных таким образом, чтобы этапы кэширования выполнялись до дополнения данных! В противном случае ваши данные дополнятся, а затем будут сохранены в таком состоянии, а это противоречит цели.

Классификация говорит, есть ли кот на картинке, а сегментация показывает где именно. Модели классификации дают ответ в форме <<Да, где-то в этой огромной куче пикселей есть кот>> или <<Нет, здесь нет котов>> \cite[\strbook{440}]{pytorch-2022}.

\subsection{Архитектура U-Net}

Архитектура U-Net -- это предназначенный для сегментации дизайн нейронной сети, который может производить попиксельные выходные данные. Данные проходят сверху слева к центру через серию сверток и масштабирования. Затем мы снова поднимаемся вверх, используя \emph{развертку}, чтобы вернуться к полному разрешению.

Однако в более ранних проектах у таких сетей были проблемы со сходимостью, скорее всего, из-за потери пространственной информации во время субдискритизации. Чтобы решить эту проблему, авторы U-Net добавили \emph{пропущенные соединения}. В U-Net пропуещенные соединения связывают входы на пути \emph{понижения дискретизации} с соответствующими слоями на пути \emph{повышения дискретизации}. Эти слои получают в качестве входных данных как результаты повышения частоты дискретизации слоев широкого рецептивного поля из нижних слоев U, так и выходные данные более ранних слоев с мелкими деталями через мостовые соединения вида <<копировать и обрезать>>. Это ключевое нововведение U-Net. В результате окончательные слои детализации берут лучшее из обоих источников данных. У них есть информация о более широком контексте, окружающем непосредственную область, а также подробные данные из первого набора слоев с полным разрешением. Слой $conv \, 1 \times 1$ в крайнем правом углу сети изменяет количество каналов с 64 на 2 \cite[\strbook{445}]{pytorch-2022}.

Вместо того чтобы просто выводить бинарную классификацию, которая дает вывод в виде <<истина>> или <<ложь>>, мы реализуем U-Net, чтобы получить модель, способную выводить значение вероятности для каждого пикселя, то есть выполнять сегментацию.

\subsubsection{Адаптация готовой модели}

Мы будем передавать входные данные через пакетную нормализацию. Тогда нам не придется самостоятельно нормализовать данные в наборе данных; и что более важно, мы получим статистику нормализации (среднее значение и стандартное отклонение), расчитанную по отдельным пакетам. 

Случайный выбор элементов данных в пакетах в каждую эпоху сводит к минимуму вероятность того, что скучный элемент окажется в полностью скучном пакете, и, следовательно, такие элементы будут рассматриваться чрезмерно внимательно.

Во-вторых, поскольку выходные значения не ограничены, мы должны пропустить выходные данные через слой \verb|nn.Sigmoid|, чтобы ограничить их диапазоном $[0, 1]$. В-третьих, мы уменьшим общую глубину и количество фильтров, которые модель будет применять. Это значит, что мы вряд ли найдем предварительно обученную модель, которая будет точно соответствовать нашим потребностям. Выходные данные получаются одноканальными, где каждый пиксель вывода содержит оценку вероятности того, что он является частью узелка.

\subsubsection{Особые требования U-Net к размеру входных данных}

Первая проблема заключается в том, что размеры входных и выходных фрагментов данных в U-Net очень специфичны. В документе U-Net использовались фрагменты изображений размером $572 \times 572$, что дает выходные карты размером $388 \times 388$. Входные изображения должны быть больше, чем наши срезы $512 \times 512$, а выходные данные оказываются чуть меньше! Это означало бы, что узелки вблизи края среза КТ вообще не будут сегментированы.

\subsubsection{Компромиссы U-Net при работе с 3D- и 2D-данными}

Вторая проблема заключается в том, что наши 3D-данные не совсем совпадают с ожидаемыми U-Net двумерными входными данными. Просто взять наше изображение размером $512 \times 512 \times 128$ и передать его в преобразованный в 3D класс U-Net не получиться, поскольку на это хватит памяти графического процессора. Каждое изображение имеет размеры $2^9 \times 2 \times ^9 \times 2^7$, по 2 байта на каждый воксель. Первый уровень U-Net -- это 64 канала, или $2^6$. Это показатель степени $9 + 9 + 7 + 2 + 6 = 33$, или 8 Гб, только для первого сверточного слоя. У нас 2 сверточных слоя (16 Гб), а затем каждое понижение разрешения уменьшает разрешение вдвое, но удваивает каналы, то есть еще 2 Гб для каждого слоя после первого понижения разрешения. В итоге мы дошли до 20 Гб еще до того, как достигли второго понижения разрешения.

Каждый срез будем рассматривать как задачу двумерной сегментации, а в качестве контекста третьего измерения станем передвать соседние срезы как отдельные каналы. Вместо традиционных красных, зеленых и синих каналов, используемых в фотографиях, нашими каналами будут <<на два среза выше>>, <<на один срез выше>>, <<сегментируемый срез>>, <<на один срез ниже>> и т.д. \cite[\strbook{451}]{pytorch-2022}

Однако, мы теряем прямую пространственную связь между срезами, когда они передаются в виде каналов, поскольку все каналы будут линейно объединены ядрами свертки без указания того, с какой стороны и на каком расстоянии находятся срезы. Еще один момент, который следует учитывать и в текущем, и трехмерном подходе, заключается в том, что теперь мы игнорируем точную толщину среза. 

Для ввода в нашу модель классификации мы рассматривали эти срезы как трехмерный массив данных и использовали трехмерные свертки для обработки каждого элемента. В модели сегментации мы будем рассматривать каждый срез как один канал и создавать многоканальное 2D-изображение. Каждый срез компьютерной томографии это как бы цветовой канал изображения RGB. Входные срезы КТ будут сложены вместе и использоваться так же, как и любое другое 2D-изображение. Каналы совмещенного КТ-изображения не будут соответствовать цветам, но в 2D-свертках и не требуется, чтобы входные каналы были именно цветами.

Вместо полных срезов КТ мы будем выполнять обучение на фрагментах $64 \times 64$ вокруг положительных кандидатов (настоящих узелков). В качестве дополнительных <<каналов>> для 2D-сегментации мы также включим по три фрагмента сверху и снизу.

Этот подход позволяет сделать обучение более стабильным и ускорить сходимость. Мы считаем, что обучение с помощью всего фрагмента оказалось нестабильным из-за проблемы с \emph{балансировкой классов}. Поскольку каждый узелок весьма мал по сравнению со всем срезом КТ, мы снова оказались в ситуации <<иголка в стоге сена>>, когда положительные элементы данных были завалены отрицательными. В данном случае мы говорим о пикселях, а не об узелках, но суть та же. Выполняя обучение на фрагментах, мы сохраняем то же количество положительных пикселей (относящихся к узелкам), но уменьшаем количество отрицательных пикселей на несколько порядков \cite[\strbook{464}]{pytorch-2022}.

Поскольку наша модель сегментации является попиксельной и принимает изображения произвольного размера, мы можем обойтись без обучения и проверки на выборках с разными размерами. 

Для этого подхода важно отметить, что, поскольку, проверочный набор содержит на несколько порядков больше отрицательных пикселей, во время проверки модель будет выдавать огромный процент ложноположительных результатов.

Необходимо устранять узкие места в конвейере обучения. Хитрость заключается в том, чтобы убедиться, что узкое место находится в самом дорогом и трудном для обновления ресурсе и использование этого ресурса не является расточительным.

Часто узкие места возникают в следующих процессах \cite[\strbook{466}]{pytorch-2022}:
\begin{itemize}
	\item в конвейере загрузки данных во время работы с необработанным вводом-выводом или при распаковке данных, когда они находятся в ОЗУ. Можно решить эту проблему с помощью библиотеки \verb|diskcache|,
	
	\item при предварительной обработке загружаемых данных в процессоре. Обычно это \emph{нормализация} или \emph{дополнение данных},
	
	\item в цикле обучения на ГП. Обычно нам лучше иметь узкое место именно здесь, поскольку общие затраты на операции глубокого обучения у графических процессоров обычно выше, чем у хранилища или ЦП,
	
	\item реже узким местом становится пропускная способность памяти между ЦП и ГП. Это означает, что ГП выполняет не так уж много работы по сравнению с объемом перемещаемых данных.
\end{itemize}

Поскольку графические процессоры при выполнении определенного класса задач могут быть в 50 раз быстрее, чем центральные, часто имеет смысл переносить эти задачи на ГП с ЦП, если последний слишком сильно нагружается. Это особенно верно в случаях, когда данные дополняются (аугментируются) во время этой обработки, так как при перемещении меньшего объема входных данных в графический процессор дополнительные данные остаются для него локальными, поэтому приходится перемещать меньше данных.

В нашем случае мы собираемся перенести дополнение (аугментацию) данных на ГП. Это снизить нагрузку на ЦП, а ГП легко справится с дополнительной нагрузкой. Лучше нагрузить ГП небольшой дополнительной работой, чем наблюдать его простой, пока процессор будет дополнять данные.

Оптимизатор Adam поддерживает выбор отдельной скорости обучения для каждого параметра и автоматически изменяет ее по мере прохождения обучения. Для большинства проектов на начальном этапе Adam считается хорошим оптимизатором. Обычно можно найти конфигурацию стохастического градиентного спуска с импульсом Нестерова, которая будет работать лучше Adam, но подобрать правильные гиперпараметры для инициализации SGD в данном проекте может быть сложно и на это уйдет много времени \cite[\strbook{471}]{pytorch-2022}.

Существует множество вариаций Adam -- AdaMax, RAdam, Ranger etc.

\subsubsection{Коэффициент Серенсена-Дайса}

\emph{Коэффициент Серенсена-Дайса}, также известный как \emph{потеря Дайса}, служит в качестве метрики потерь в задачах сегментации. 

Одним из преимуществ использования потери Дайса, по сравнению с перекрестно-энтропийной потерей на пиксель, является то, что потеря Дайса обрабатывает случай, когда лишь небольшая часть изображения помечена положительной. При использовании кросс-энтропийных потерь несбалансированные обучающие данные создают проблемы. Здесь у нас именно такая ситуация -- большая часть КТ не является узелком. К счастью, при использовании потери Дайса это не проблема.

Коэффициент Серенсена-Дайса -- это отношение правильно сегментированных пикселей к сумме предсказанных и фактических пикселей. Это удвоенная площадь пересечения (истинно положительные результаты), деленная на сумму всей прогнозируемой площади и всей области, помеченной как истинная (пересечение считается дважды) \cite[\strbook{472}]{pytorch-2022}.

По сути, мы собираемся для каждого пикселя использовать метрику F1. Поскольку мы хотим, чтобы потери были минимальными, мы возьмем полученное отношение и вычтем его из 1. Это инвертирует наклон функции потерь, так что в случае большого перекрытия наши потери будут низкими и при низком перекрытии -- высокими. 

Мы готовы пожертвовать огромным количеством истинных отрицательных пикселей в погоне за лучшим откликом, поэтому в целом стоит ожидать большого количества ложных срабатываний. Все дело в том, что в нашей задаче очень-очень важен хороший отклик, и мы бы предпочли получить несколько ложноположительных результатов, чем хотя бы один ложноотрицательный.

Следует заметить, что этот подход работает только в случае применения оптимизатора Adam. При использовании SGD толчок к завышению прогноза приведет к тому, что каждый пиксель начнет определяться как положительный. 

Поскольку точные значения наших прогнозов нас не слишком волнуют (не имеет значения, равна вероятность пикселя 0,6 или 0,9, так как для рассмотрения его как кандидата достаточно просто превысить пороговое значение), мы собираемся сравнивать его с порогом 0,5.

<<Под капотом>> функция \verb|torch.save()| используется стандартная библиотека Python \verb|pickle|, которой можно напрямую передать экземпляр модели, и он сохраниться правильно. Однако это не считается идеальным спобсом сохранения нашей модели, поскольку он лишает нас некой гибкости.

Вместо этого мы сохраним только \emph{параметры} модели. Так мы получим возможнсоть загружать эти параметры в любую модель, способную принимать параметры той же формы, даже если класс не соответствует модели, в которой были сохранены эти параметры. Подход с сохранением только параметров позволяет нам повторно использовать и смешивать модели более гибко, чем если бы мы сохраняли сразу всю модель \cite[\strbook{481}]{pytorch-2022}.

\subsection{Независимость проверочного набора}

Существует потенциальный риск утечки данных из обучающего набора в проверочный! В каждой из моделей сегментации и классификации мы выполняли разделение данных на обучающий набор и независимый проверочный набор, отобрав каждый десятый пример для проверки, а остальные -- для обучения.

Однако разделение для модели классификации выполнялось по списку узелков, а разделение для модели сегментации -- по списку КТ. В результате, вероятно, некоторые узелки из проверочного набора сегментации попали в обучающий набор модели классификации и наоборот. Этого необходимо избежать! Если не исправить такую ситуацию, то показатели эффективности модели могут оказаться искусственно завышенными по сравнению с тем, что мы получим на независимом наборе данных. Это явление называется \emph{утечкой}, и оно делает всю процедуру проверки недействительной.

При определении проверочного набора нужно смотреть на весь процесс целиком. Вероятно, самый простой способ сделать это (для важных наборов данных так и происходит) -- реализовать \emph{\color{blue}разделение данных проверки как можно более \emph{явно}}, например используя два отдельных каталога для \emph{обучающих} и \emph{проверочных данных}, а затем придерживаться этого разделения на протяжении всего проекта \cite[\strbook{492}]{pytorch-2022}.

У каждой КТ мы сегментируем каждый срез, а затем берем весь сегментированный вывод в качестве входных данных для группировки. Результат группировки будет передан в классификатор узелков, а узлы, выжившие после такой классификации, будут переданы в классификатор злокачественных новообразований.

\subsection{Объединение сегментации КТ и классификации узелков-кандидатов}

\subsubsection{Сегментация}

Прежде всего нам нужно выполнить сегментацию каждого среза всей компьютерной томографии. Поскольку КТ данного пациента передается срез за срезом, мы строим \verb|Dataset|, который загружает КТ с одним \verb|series_uid| и возвращает каждый ее срез, по одному на вызов \verb|__getitem__|.

Напомним: на выходе получается массив попиксельных вероятностей (то есть значений в диапазоне от 0 до 1) того, что данный пиксель является частью узелка. Перебирая срезы, мы собираем прогнозы по срезам в массиве масок, который имеет ту же форму, что и входные данные КТ. После этого мы выставляем пороговое значение, чтобы получить двоичный массив. Мы будем использовать пороговое значение 0.5, но можно было бы и поэкспериментировать с ним, чтобы получить больше истинно положительных результатов в обмен на увеличение ложноотрицательных.

С помощью функции \verb|scipy.ndimage.morphology.binary_erosion()| удобно выполнять эрозию -- удалять слои граничных элементов матрицы
\begin{lstlisting}[
style = ironpython,
numbers = none
]
from scipy.ndimage.morphology import binary_erosion

def make_erosion(image: np.ndarray, iterations: int = 1) -> np.ndarray:
    return binary_erosion(image, iterations=iterations)
\end{lstlisting}

Связанные компоненты можно пометить с помощью функции \verb|label|
\begin{lstlisting}[
style = ironpython,
numbers = none
]
from scipy.ndimage.measurements import label

labled_array, n_features = label(image)
\end{lstlisting}

Обратите внимание, что случайные прогнозы на \emph{сбалансированном} наборе данных дают значение $AUC = 0.5$, так что наш классификатор должен по крайней мере превосходить это значение.

\subsubsection{Повторное использование весов: тонкая настройка}

Один из способов быстро получить результаты (а зачастую и обойтись гораздо меньшим объемом данных) -- не задавать веса случайно, а взять сеть, обученную какой-либо задаче со связанными данными. Это называется \emph{передачей обучения} или, когда речь идет об обучении нескольких последних слоев, \emph{тонкой настройкой}. 

То есть мы рассматриваем некоторую (часто большую) часть сети как некий \emph{распознаватель признаков}, а обучаем только относительно небольшую часть поверх нее.

Обычно это прекрасно работает. Переобучим \verb|head_linear| и последний сверточный блок:
\begin{itemize}
	\item загрузить нужные начальные веса модели, за исключением последнего линейного слоя, в котором нужна случайная инициализация,
	
	\item {\color{blue}отключить градиенты для параметров, которые мы \emph{не хотим обучать} (все кроме параметров с именами, начинающимися со слова \verb|head|)}.
\end{itemize}

\begin{lstlisting}[
style = ironpython,
numbers = none
]
d = torch.load(self.cli_args.finetune, map_location="cpu")
model_blocks = [
    n for n, subm in model.named_children()
    if len(list(subm.parameters())) > 0
]

finetune_blocks = model_blocks[-self.cli_args.finetune_depth:]
model.load_state_dict(
    {
        k: v for k, v in d["model_state"].items()
        if k.split(".")[0] not in model_blocks[-1]
    },
    strict=False
)
for n, p in model.named_parameters():
    if n.split(".")[0] not in finetune_blocks:
        # для всех, кроме finetune_blocks, градиенты не нужны
        p.requires_grad_(False)
\end{lstlisting}

Если обучения только полносвязной части в методе тонкой настройки недостаточно, то нужно попробовать включить в обучение последний сверточный блок \cite[\strbook{512}]{pytorch-2022}.

Если сеть становится слишком самоуверенной, то можно попробовать просто не применять однократное распределение, а вместо этого присвоить небольшую вероятностную массу <<неправильным>> классам. Для этого можно использовать \verb|nn.KLDivLoss| \cite[\strbook{523}]{pytorch-2022}.

HTTP старой школы \emph{глубоко последователен}, то есть, когда клиент хочет отправить несколько запросов в одном и том же подключении, следующий запрос отправляется только после того, как будет получен ответ на предыдущий \cite[\strbook{538}]{pytorch-2022}.

При использовании графических процессоров часто гораздо эффективнее обрабатывать запросы \emph{пакетами}, чем запускать их один за другим или параллельно.

\subsection{Встроенный механизм экспорта PyTorch: отслеживание}

Если совместимость не столь важна, но все еще нужно обойти Python GIL или иным образом экспортировать сеть, мы можем использовать собственное представление PyTorch, называемое TorchScript graph.

Самый простой способ создать модель TorchScript -- отследить ее. Этот механизм работает точно таке же, как экспорт ONNX. Здесь мы просто подаем в модель фиктивные входные данные с помощью функции \verb|torch.jit.trace|. Загружаем обученные параметры и переводим модель в режим оценки. Прежде чем выполнить отслеживание модели, дадим еще одно предостережение: \emph{\color{blue} ни один из параметров не должен требовать градиентов}. Даже если мы проследим модель внутри \verb|no_grad|, а затем запустим ее снаружи, PyTorch запишет градиенты.

\begin{lstlisting}[
style = ironpython,
numbers = none
]
import torch

seg_dict = torch.load(".../best.state", map_location="cpu")

seg_model = UNetWrapper(in_channels=8, n_classes=1, depth=4, wf=3, padding=True, batch_norm=True, up_mode="upconv")
seg_model.load_state_dict(seg_dict["model_state"])
seg_model.eval()
for p in seg_model.parameters():
    p.requires_grad_(False)
    
dummy_input = torch.randn(1, 8, 512, 512)
traced_seg_model = torch.jit.trace(seg_model, dummy_input)  # отслеживание
\end{lstlisting}

Можно сохранить отслеженную модель
\begin{lstlisting}[
style = ironpython,
numbers = none
]
torch.jit.save(traced_seg_model, "traced_seg_model.pt")
\end{lstlisting}
и загрузить ее обратно с сохраненным файлом
\begin{lstlisting}[
style = ironpython,
numbers = none
]
loaded_model = torch.jit.load("traced_seg_model.pt")
prediction = loaded_model(batch)
\end{lstlisting}

PyTorch JIT запомнит состояние модели в момент сохранения, то есть настройку в режим оценки и отключение требования градиентов.

Существует два простых способа создания модели TorchScript \cite[\strbook{553}]{pytorch-2022}: 
\begin{itemize}
	\item отслеживание -- это выполнение обычно модели PyTorch на случайных входных данных.
	
	\item создание сценариев: сценарии PyTorch JIT просматривают фактический код Python, отвечающий за вычисления, и компилируют его в TorchScript IR.
\end{itemize}








% Источники в "Газовой промышленности" нумеруются по мере упоминания 
\begin{thebibliography}{99}\addcontentsline{toc}{section}{Список литературы}
	
	\bibitem{pytorch-2022}{\emph{Стивенс Э.} PyTorch. Освещая глубокое обучение. -- СПб.: Птер, 2022. -- 576 с.}
\end{thebibliography}

%\listoffigures\addcontentsline{toc}{section}{Список иллюстраций}

%\lstlistoflistings\addcontentsline{toc}{section}{Список листингов}

\end{document}
